{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:41219' processes=10 threads=10, memory=416.00 GiB>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-7bd697bf-3f7c-11f0-bd22-0000056ffe80</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/proxy/8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">1f27f1cb</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 10\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 10\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 416.00 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-7ba551a7-4897-40f1-adcb-88215c51da2e</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:41219\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 10\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 10\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 416.00 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:46815\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/45019/status\" target=\"_blank\">/proxy/45019/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:41999\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-ixvlfvtz\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:43001\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/38851/status\" target=\"_blank\">/proxy/38851/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:33639\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-e46lmot5\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 2</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:42365\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/35749/status\" target=\"_blank\">/proxy/35749/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:42563\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-rrqyocbw\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 3</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:39441\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/39847/status\" target=\"_blank\">/proxy/39847/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:43619\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-04lvy4_i\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 4</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:46575\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/43085/status\" target=\"_blank\">/proxy/43085/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:44787\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-rkptbmbz\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 5</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:36607\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/45747/status\" target=\"_blank\">/proxy/45747/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:38281\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-3qke1iqn\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 6</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:45043\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/44759/status\" target=\"_blank\">/proxy/44759/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:36735\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-a_ck0s9j\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 7</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:37395\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/38861/status\" target=\"_blank\">/proxy/38861/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:35005\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-08poompz\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 8</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:33075\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/36123/status\" target=\"_blank\">/proxy/36123/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:34521\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-hfzliosg\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 9</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:39915\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/33679/status\" target=\"_blank\">/proxy/33679/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 41.60 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:36075\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/142199311.gadi-pbs/dask-scratch-space/worker-jao04ian\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:41219' processes=10 threads=10, memory=416.00 GiB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 16:45:16,221 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45043 (pid=1621334) exceeded 95% memory budget. Restarting...\n",
      "2025-06-02 16:45:16,409 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:45043' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('getitem-180eae8f4654c185e14c8c236b2217a3', 0, 0, 0, 0, 6), ('transpose-ca14a990a0a95db6f4eb587b5137c3b2', 4, 0, 0, 0), ('transpose-be369296c5bbfd7d0c0778d56e5ec49c', 2, 0, 0), ('where-4a78198bb5af05db46bafcf0b7e4c54d', 0, 0, 0, 0, 5), ('transpose-be369296c5bbfd7d0c0778d56e5ec49c', 0, 0, 0), ('getitem-98663746d7a1f500d952170d9a02cf39', 0, 0, 0, 0, 6)} (stimulus_id='handle-worker-cleanup-1748846716.4085002')\n",
      "2025-06-02 16:45:16,426 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-06-02 16:50:14,065 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:46815 (pid=1621311) exceeded 95% memory budget. Restarting...\n",
      "2025-06-02 16:50:14,468 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:46815' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('getitem-3408bf54cc08bcc6832d9c8d9829cbd1', 1, 0, 0, 0, 1), ('transpose-5e2c5678429e92031ff32aca943aad8d', 4, 0, 0, 0), ('getitem-3408bf54cc08bcc6832d9c8d9829cbd1', 1, 0, 0, 0, 13), ('getitem-c17abf33e6fc470343ad74059a6d9aaf', 1, 0, 0, 0, 1), ('getitem-c17abf33e6fc470343ad74059a6d9aaf', 1, 0, 0, 0, 13)} (stimulus_id='handle-worker-cleanup-1748847014.4672935')\n",
      "2025-06-02 16:50:14,740 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-06-02 16:54:34,223 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:33075 (pid=1621343) exceeded 95% memory budget. Restarting...\n",
      "2025-06-02 16:54:34,387 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:33075' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('transpose-ca14a990a0a95db6f4eb587b5137c3b2', 1, 0, 0, 0), ('where-40e8b4be8d506bbcb2405370229a77fd', 1, 0, 0, 0, 1), ('getitem-c71b651e13802af96d3e9b362c1ce31e', 1, 0, 0, 0, 7), ('getitem-95d09de3a3cdf8b3a45909de513580d1', 1, 0, 0, 0, 7), ('transpose-ca14a990a0a95db6f4eb587b5137c3b2', 5, 0, 0, 0)} (stimulus_id='handle-worker-cleanup-1748847274.3871086')\n",
      "2025-06-02 16:54:34,440 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-06-02 17:00:22,969 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43001 (pid=1621313) exceeded 95% memory budget. Restarting...\n",
      "2025-06-02 17:00:23,109 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:43001' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('getitem-95d09de3a3cdf8b3a45909de513580d1', 2, 0, 0, 0, 0), ('getitem-c71b651e13802af96d3e9b362c1ce31e', 2, 0, 0, 0, 0), ('transpose-ca14a990a0a95db6f4eb587b5137c3b2', 2, 0, 0, 0), ('where-40e8b4be8d506bbcb2405370229a77fd', 2, 0, 0, 0, 5)} (stimulus_id='handle-worker-cleanup-1748847623.1082635')\n",
      "2025-06-02 17:00:23,153 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/149/ab8992/tasman-tides/\")\n",
    "import xarray as xr\n",
    "import ttidelib as tt\n",
    "import scipy\n",
    "import cmocean\n",
    "import os\n",
    "from pathlib import Path\n",
    "cmap = cmocean.cm.dense_r\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "earth_cmap = matplotlib.colormaps[\"gist_earth\"]\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# import filtering\n",
    "import numpy as np\n",
    "import dask\n",
    "dask.config.set({'logging.distributed': 'error'})\n",
    "from dask.distributed import Client,default_client\n",
    "import xrft\n",
    "\n",
    "\n",
    "client = tt.startdask(nthreads=1,n_workers = 10)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpstorage = os.getenv('PBS_JOBFS')\n",
    "tmpstorage = \"/scratch/nm03/ab8992/test\"\n",
    "outputdir = \"/scratch/nm03/ab8992/test/outputs\"\n",
    "def DirectionalFilter(data,dim = \"xb\"):\n",
    "    \"\"\"\n",
    "    Fourier filter into forward and backward propagating signals\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if \"lat\" in data.coords:\n",
    "        data = data.drop([\"lat\",\"lon\"])\n",
    "    import xrft\n",
    "    FT = xrft.fft(\n",
    "        data,dim = [\"time\",dim]\n",
    "    )\n",
    "\n",
    "    forward = np.real(xrft.ifft(\n",
    "        FT.where((FT[f\"freq_{dim}\"] > 0) & (FT.freq_time > 0), 0) +\n",
    "          FT.where((FT[f\"freq_{dim}\"] < 0) & (FT.freq_time < 0), 0),\n",
    "        dim = [\"freq_time\",f\"freq_{dim}\"]\n",
    "    ))\n",
    "\n",
    "    backward = np.real(xrft.ifft(\n",
    "        FT.where((FT[f\"freq_{dim}\"] < 0) & (FT.freq_time > 0), 0) + \n",
    "        FT.where((FT[f\"freq_{dim}\"] > 0) & (FT.freq_time < 0), 0),\n",
    "        dim = [\"freq_time\",f\"freq_{dim}\"]\n",
    "    ))\n",
    "\n",
    "    return xr.merge([forward.rename(f\"{data.name}_forward\"),backward.rename(f\"{data.name}_backward\")]).assign_coords({\"time\":data.time,dim:data[dim]})\n",
    "def laplacian(data):\n",
    "    out = data.u.bfill(\"xb\",limit = 4).ffill(\"xb\",limit = 4).differentiate(\"xb\").differentiate(\"xb\")\n",
    "    out +=data.v.bfill(\"yb\",limit = 4).ffill(\"yb\",limit = 4).differentiate(\"yb\").differentiate(\"yb\")\n",
    "    return out\n",
    "\n",
    "def _scipy_integrate(data, zl):\n",
    "    \"\"\"\n",
    "    Helper function to perform cumulative trapezoidal integration along the 'zl' axis.\n",
    "    \"\"\"\n",
    "    return scipy.integrate.cumulative_trapezoid(data, x=zl, initial=0)\n",
    "\n",
    "def scipy_integrate(data):\n",
    "    \"\"\"\n",
    "    Wrapper function to apply cumulative trapezoidal integration along the 'zl' axis.\n",
    "    \"\"\"\n",
    "    zl = data.zl.values  # Extract 'zl' coordinate values\n",
    "    return np.apply_along_axis(_scipy_integrate, data.get_axis_num('zl'), data, zl=zl)\n",
    "\n",
    "def save_temporary(expt,t0,outputdir):\n",
    "    \"\"\"\n",
    "    Open all the datasets that we need for this experiment. Save some of the simple stuff, and save the modal decompossitions to temporary storage chunked by mode for further processing.\n",
    "    \"\"\"\n",
    "    with (\n",
    "            xr.open_mfdataset(f\"/g/data/nm03/ab8992/postprocessed/{expt}/bandpassed/t0-{t0}/Filtered*.nc\",decode_times = False) as filtered,\n",
    "            tt.collect_data(expt,rawdata=[\"u\",\"v\",\"ahh\"],timerange = (filtered.time.values[0],filtered.time.values[-1])).isel(zl = slice(0,96)) as raw\n",
    "\n",
    "    ):\n",
    "        if \"80\" in expt: #! vmodes have failed for some times. For now, fix vmodes to match for each run\n",
    "            vmodes= xr.open_dataset(f\"/g/data/nm03/ab8992/postprocessed/{expt}/vertical_eigenfunctions/vmode-t0-{12258}.nc\",decode_times = False,chunks = {\"mode\":1}).isel(zl = slice(0,96))\n",
    "            vmodes = vmodes.interp_like(raw.u.isel(time = 0)).persist() ## Now need to chunk this on disk like the Filtered data\n",
    "            print(\"vmodes interpolated\")\n",
    "            ## Fix messed up times:\n",
    "            filtered = filtered.assign_coords({\"time\":np.arange(0,len(filtered.time.values))})\n",
    "        elif \"40\" in expt: #! vmodes have failed for some times. For now, fix vmodes to match for each run\n",
    "            vmodes = xr.open_dataset(f\"/g/data/nm03/ab8992/postprocessed/{expt}/vertical_eigenfunctions/vmode-t0-{4216}.nc\",decode_times = False,chunks = {\"mode\":1}).isel(zl = slice(0,96))\n",
    "        else:\n",
    "            vmodes = xr.open_dataset(f\"/g/data/nm03/ab8992/postprocessed/{expt}/vertical_eigenfunctions/vmode-t0-{22000}.nc\",decode_times = False,chunks = {\"mode\":1}).isel(zl = slice(0,96))\n",
    "\n",
    "            \n",
    "        # os.remove(f\"{tmpstorage}/*.nc\")\n",
    "        vmodes = vmodes.assign_coords({\"zl\":filtered.zl})\n",
    "        ymin = vmodes.yb[0].values - 0.0001 ## This ensures random numerical changes of 1e-16 to axis values don't cause issues... \n",
    "        ymax = vmodes.yb[-1].values + 0.0001\n",
    "        raw = raw.sel(yb = slice(ymin,ymax)).assign_coords(vmodes.isel(mode = 0).coords).drop_vars([\"lat\",\"lon\"])\n",
    "        filtered = filtered.sel(yb = slice(ymin,ymax)).assign_coords(vmodes.isel(mode = 0).coords)\n",
    "\n",
    "\n",
    "        ## Here all variables are dask arrays. One by one, compute what we want and save to PBS temporart storage \n",
    "\n",
    "        for i in range(len(vmodes.mode.values)):\n",
    "            # print(\"Saving mode \",i)\n",
    "            (vmodes.U * filtered.u).fillna(0).integrate(\"zl\").rename(\"u\").isel(mode = [i]).to_netcdf(f\"{tmpstorage}/u_{i}.nc\")\n",
    "            (vmodes.U * filtered.v).fillna(0).integrate(\"zl\").rename(\"v\").isel(mode = [i]).to_netcdf(f\"{tmpstorage}/v_{i}.nc\")\n",
    "            (vmodes.W * filtered.rho).fillna(0).integrate(\"zl\").rename(\"rho\").isel(mode = [i]).to_netcdf(f\"{tmpstorage}/rho_{i}.nc\")\n",
    "\n",
    "        #! OLD VERSION: I was missing the depth in the cumsum. this should be a proper integral, \n",
    "        #! so needed to multiply by cell thicknesses\n",
    "        #! (\n",
    "        #!     vmodes.U * vmodes.W.cumsum(\"zl\") * 9.8\n",
    "        #!     ).fillna(0).integrate(\"zl\").rename(\"p_coeff\").to_netcdf(f\"{tmpstorage}/p_coeff.nc\")        \n",
    "\n",
    "        #! Updated: Now use the scipy integrate function to do cumsum correctly, retaining cell thicknesses\n",
    "        (\n",
    "            vmodes.U * scipy_integrate(vmodes.W) * 9.8\n",
    "            ).fillna(0).integrate(\"zl\").rename(\"p_coeff\").to_netcdf(f\"{tmpstorage}/p_coeff.nc\") \n",
    "\n",
    "\n",
    "        ## While we have files open, might as well save some other simple stuff, like vorticity in top 20 layers, total raw KE, both bc and bt.\n",
    "        # print(\"Save vorticity...\")\n",
    "\n",
    "        ((raw.v.differentiate(\"xb\") - raw.u.differentiate(\"yb\")).fillna(0).mean(\"time\").sel(zl = slice(0,200)).integrate(\"zl\") / 200).rename(\"vorticity\").to_netcdf(f\"{outputdir}/raw_vorticity.nc\")\n",
    "\n",
    "        print(\"Save raw KE...\")\n",
    "        (raw.u**2 + raw.v**2).fillna(0).integrate(\"zl\").mean(\"time\").rename(\"raw_KE_total\").to_netcdf(f\"{outputdir}/raw_ke_total.nc\")\n",
    "        # print(\"Save raw KE bt...\")\n",
    "        (\n",
    "            (raw.u.fillna(0).integrate(\"zl\")**2 + raw.v.fillna(0).integrate(\"zl\")**2) / raw.bathy\n",
    "        ).mean(\"time\").rename(\"raw_KE_bt\").to_netcdf(f\"{outputdir}/raw_ke_bt.nc\")\n",
    "        # print(\"Save raw dissipation...\")\n",
    "        # (\n",
    "        #     (raw.ahh * laplacian(raw)**2).mean(\"time\") / (1e6)\n",
    "        # ).fillna(0).integrate(\"zl\").rename(\"raw_dissipation\").to_netcdf(f\"{outputdir}/raw_dissipation.nc\")\n",
    "\n",
    "        # print(\"Save filtered KE...\")\n",
    "        (filtered.u**2 + filtered.v**2).fillna(0).integrate(\"zl\").mean(\"time\").rename(\"filtered_KE_total\").to_netcdf(f\"{outputdir}/filtered_ke_total.nc\")\n",
    "        # print(\"Save filtered KE bt...\")\n",
    "        (\n",
    "            (filtered.u.fillna(0).integrate(\"zl\")**2 + filtered.v.fillna(0).integrate(\"zl\")**2) / raw.bathy\n",
    "        ).mean(\"time\").rename(\"filtered_KE_bt\").to_netcdf(f\"{outputdir}/filtered_ke_bt.nc\")\n",
    "        print(\"Save filtered dissipation...\")\n",
    "        # (\n",
    "        #     (raw.ahh * laplacian(filtered)**2).mean(\"time\") / (1e6)\n",
    "        # ).fillna(0).integrate(\"zl\").rename(\"filtered_dissipation\").to_netcdf(f\"{outputdir}/filtered_dissipation.nc\")        \n",
    "\n",
    "    return\n",
    "\n",
    "def EF_from_u_rho(u,rho,p_coeff): #! TODO MAKE SURE THIS WORKS FOR NON FILTERED EF!\n",
    "    if isinstance(rho, xr.DataArray):\n",
    "        EF = rho * u * p_coeff\n",
    "        return EF.rename(\"EF\")\n",
    "\n",
    "    EF_forward = rho[f\"rho_forward\"] * u[f\"u_forward\"] * p_coeff\n",
    "    EF_backward = rho[f\"rho_backward\"] * u[f\"u_backward\"] * p_coeff\n",
    "    EF_cross = rho[f\"rho_backward\"] * u[f\"u_forward\"] * p_coeff\n",
    "    EF_cross += rho[f\"rho_forward\"] * u[f\"u_backward\"] * p_coeff\n",
    "    return xr.merge([EF_forward.rename(\"EF_alongbeam_forward\"),EF_backward.rename(\"EF_alongbeam_backward\"),EF_cross.rename(\"EF_alongbeam_xterm\")]).mean(\"time\")\n",
    "\n",
    "def EF_from_v_rho(v,rho,p_coeff): #! TODO MAKE SURE THIS WORKS FOR NON FILTERED EF!\n",
    "    if isinstance(rho, xr.DataArray):\n",
    "        EF = rho * v * p_coeff\n",
    "        return EF.rename(\"EF\")\n",
    "\n",
    "    EF_forward = rho[f\"rho_forward\"] * v[f\"v_forward\"] * p_coeff\n",
    "    EF_backward = rho[f\"rho_backward\"] * v[f\"v_backward\"] * p_coeff\n",
    "    EF_cross = rho[f\"rho_backward\"] * v[f\"v_forward\"] * p_coeff\n",
    "    EF_cross += rho[f\"rho_forward\"] * v[f\"v_backward\"] * p_coeff\n",
    "    return xr.merge([EF_forward.rename(\"EF_acrossbeam_forward\"),EF_backward.rename(\"EF_acrossbeam_backward\"),EF_cross.rename(\"EF_acrossbeam_xterm\")]).mean(\"time\")\n",
    "\n",
    "def save_modal(outputdir):\n",
    "    U = xr.open_mfdataset(f\"{tmpstorage}/u_*.nc\",combine = \"by_coords\",decode_times = False).u\n",
    "    V = xr.open_mfdataset(f\"{tmpstorage}/v_*.nc\",combine = \"by_coords\",decode_times = False).v\n",
    "    rho = xr.open_mfdataset(f\"{tmpstorage}/rho_*.nc\",combine = \"by_coords\",decode_times = False).rho\n",
    "    p_coeff = xr.open_mfdataset(f\"{tmpstorage}/p_coeff.nc\",combine = \"by_coords\",decode_times = False).p_coeff\n",
    "\n",
    "    ## EF unfiltered\n",
    "    EF_from_u_rho(U,rho,p_coeff).to_netcdf(f\"{outputdir}/EF_alongbeam.nc\")\n",
    "    EF_from_v_rho(V,rho,p_coeff).to_netcdf(f\"{outputdir}/EF_acrossbeam.nc\")\n",
    "\n",
    "    ## EF filtered\n",
    "    print(\"saving EF\")\n",
    "    with EF_from_u_rho(DirectionalFilter(U),DirectionalFilter(rho),p_coeff) as EF:\n",
    "        EF[\"EF_alongbeam_forward\"].to_netcdf(f\"{outputdir}/EF_alongbeam_forward.nc\",mode = \"w\")\n",
    "        EF[\"EF_alongbeam_backward\"].to_netcdf(f\"{outputdir}/EF_alongbeam_backward.nc\",mode = \"w\")\n",
    "        EF[\"EF_alongbeam_xterm\"].to_netcdf(f\"{outputdir}/EF_alongbeam_xterm.nc\",mode = \"w\")\n",
    "        \n",
    "    with EF_from_v_rho(DirectionalFilter(V),DirectionalFilter(rho),p_coeff) as EF:\n",
    "        EF[\"EF_acrossbeam_forward\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_forward.nc\",mode = \"w\")\n",
    "        EF[\"EF_acrossbeam_backward\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_backward.nc\",mode = \"w\")\n",
    "        EF[\"EF_acrossbeam_xterm\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_xterm.nc\",mode = \"w\")\n",
    "    return \n",
    "\n",
    "## Overwrite so we can save filtered EF\n",
    "def save_modal(outputdir):\n",
    "    U = xr.open_mfdataset(f\"{tmpstorage}/u_*.nc\",combine = \"by_coords\",decode_times = False).u\n",
    "    V = xr.open_mfdataset(f\"{tmpstorage}/v_*.nc\",combine = \"by_coords\",decode_times = False).v\n",
    "    rho = xr.open_mfdataset(f\"{tmpstorage}/rho_*.nc\",combine = \"by_coords\",decode_times = False).rho\n",
    "    p_coeff = xr.open_mfdataset(f\"{tmpstorage}/p_coeff.nc\",combine = \"by_coords\",decode_times = False).p_coeff\n",
    "\n",
    "    #! Handle stupid 80th case of times\n",
    "    time = np.arange(0,len(U.time.values))\n",
    "    U,V,rho = U.assign_coords({\"time\":time}), V.assign_coords({\"time\":time}), rho.assign_coords({\"time\":time})\n",
    "\n",
    "    ## EF unfiltered\n",
    "    EF_from_u_rho(U,rho,p_coeff).to_netcdf(f\"{outputdir}/EF_alongbeam.nc\",mode = \"w\")\n",
    "    EF_from_v_rho(V,rho,p_coeff).to_netcdf(f\"{outputdir}/EF_acrossbeam.nc\",mode = \"w\")\n",
    "    (U**2 + V**2).mean(\"time\").rename(\"KE\").to_netcdf(f\"{outputdir}/KE.nc\",mode = \"w\")\n",
    "    ## EF filtered\n",
    "    with DirectionalFilter(U, dim=\"xb\") as Uf, DirectionalFilter(rho, dim=\"xb\") as rhof, DirectionalFilter(V, dim=\"xb\") as Vf:\n",
    "        EF_along = EF_from_u_rho(Uf, rhof, p_coeff)\n",
    "        EF_across = EF_from_v_rho(Vf, rhof, p_coeff)\n",
    "        KE_forward = (Uf[\"u_forward\"]**2 + Vf[\"v_forward\"]**2).mean(\"time\").rename(\"KE_forward\")\n",
    "        KE_backward = (Uf[\"u_backward\"]**2 + Vf[\"v_backward\"]**2).mean(\"time\").rename(\"KE_backward\")\n",
    "        KE_cross = (Uf[\"u_backward\"] * Uf[\"u_forward\"] + Vf[\"v_backward\"] * Vf[\"v_forward\"]).mean(\"time\").rename(\"KE_cross\")\n",
    "\n",
    "        EF_along[\"EF_alongbeam_forward\"].to_netcdf(f\"{outputdir}/EF_alongbeam_forward.nc\", mode=\"w\")\n",
    "        EF_along[\"EF_alongbeam_backward\"].to_netcdf(f\"{outputdir}/EF_alongbeam_backward.nc\", mode=\"w\")\n",
    "        EF_along[\"EF_alongbeam_xterm\"].to_netcdf(f\"{outputdir}/EF_alongbeam_xterm.nc\", mode=\"w\")\n",
    "\n",
    "        EF_across[\"EF_acrossbeam_forward\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_forward.nc\", mode=\"w\")\n",
    "        EF_across[\"EF_acrossbeam_backward\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_backward.nc\", mode=\"w\")\n",
    "        EF_across[\"EF_acrossbeam_xterm\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_xterm.nc\", mode=\"w\")\n",
    "        KE_forward.to_netcdf(f\"{outputdir}/KE_forward.nc\", mode=\"w\")\n",
    "        KE_backward.to_netcdf(f\"{outputdir}/KE_backward.nc\", mode=\"w\")\n",
    "        KE_cross.to_netcdf(f\"{outputdir}/KE_cross.nc\", mode=\"w\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beamless-80\n",
      "80th run: adjusting outputs to [ 93  94  95  96  97  98  99 100 101 102]\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "vmodes interpolated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 16:50:14,512 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46815\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:51256 remote=tcp://127.0.0.1:46815>: Stream is closed\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "expts = [\"full-10\",\"beamless-10\",\"smooth-10\",\"beamless-20\",\"smooth-20\",\"full-20\",\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "# expts = [\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "\n",
    "t0_20th = 22000\n",
    "t0_40th = 4216\n",
    "t0_80th = 12023\n",
    "expts = [\"beamless-80\"]\n",
    "for expt in expts:\n",
    "    outputdir = \"/scratch/nm03/ab8992/april25/outputs/\"+expt\n",
    "    tmpstorage = \"/scratch/nm03/ab8992/april25/tmpstorage/\"+expt\n",
    "\n",
    "\n",
    "    if not os.path.exists(outputdir):\n",
    "        os.makedirs(outputdir)\n",
    "    if not os.path.exists(tmpstorage):\n",
    "        os.makedirs(tmpstorage)\n",
    "    print(expt)\n",
    "\n",
    "    save_temporary(expt,t0_80th,outputdir)\n",
    "    save_modal(outputdir)\n",
    "    print(\"Done with \",expt)\n",
    "    # clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over many times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special case: do the 80th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "expts = [\"full-10\",\"beamless-10\",\"smooth-10\",\"beamless-20\",\"smooth-20\",\"full-20\",\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "# expts = [\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "t0_80th = 12023\n",
    "expts = [\"full-80\"]\n",
    "for expt in expts:\n",
    "    print(f\"Starting {expt}\" )\n",
    "\n",
    "    outputdir = f\"/scratch/nm03/ab8992/april-manytimes/{expt}/{t0}\"\n",
    "    # CHeck how many files are in the directory\n",
    "    if os.path.exists(outputdir) and len(os.listdir(outputdir)) == 18:\n",
    "        print(f\"{t0} already complete.\")\n",
    "        continue\n",
    "    print(t0)\n",
    "    tmpstorage = os.getenv('PBS_JOBFS') + \"/tmpstorage\"\n",
    "    tmpstorage = \"/scratch/nm03/ab8992/april-tmpstorage/tmpstorage/\"+expt\n",
    "    if os.path.exists(tmpstorage):\n",
    "        shutil.rmtree(tmpstorage)\n",
    "    if os.path.exists(outputdir):\n",
    "        shutil.rmtree(outputdir)\n",
    "    os.makedirs(outputdir)\n",
    "    os.makedirs(tmpstorage)\n",
    "    \n",
    "    try:\n",
    "        save_temporary(expt, t0.split(\"-\")[-1], outputdir)\n",
    "        save_modal(outputdir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {expt} at time {t0}: {e}\")\n",
    "    print(\"Done with \",expt)\n",
    "    # clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full-10\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504 already complete.\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744 already complete.\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "Done with  full-10\n",
      "Starting beamless-10\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "Done with  beamless-10\n",
      "Starting smooth-10\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "t0-10001 already complete.\n",
      "Done with  smooth-10\n",
      "Starting beamless-20\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "Done with  beamless-20\n",
      "Starting smooth-20\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-10000\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "t0-10001\n",
      "Error processing smooth-20 at time t0-10001: no files to open\n",
      "Done with  smooth-20\n",
      "Starting full-20\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "Done with  full-20\n",
      "Starting full-40\n",
      "t0-3960\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:50:28,321 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.60 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:35,739 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46373\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:45496 remote=tcp://127.0.0.1:46373>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:36,905 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35791\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:46658 remote=tcp://127.0.0.1:35791>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:38,110 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39187\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:33608 remote=tcp://127.0.0.1:39187>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:38,157 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42367\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50656 remote=tcp://127.0.0.1:42367>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:44,747 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41393\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:39278 remote=tcp://127.0.0.1:41393>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:44,823 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43701\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:58628 remote=tcp://127.0.0.1:43701>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:45,482 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.20 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:45,709 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.20 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:45,783 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39201\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:52286 remote=tcp://127.0.0.1:39201>: Stream is closed\n",
      "2025-05-19 12:50:45,950 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38909\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47338 remote=tcp://127.0.0.1:38909>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:46,008 - distributed.diskutils - ERROR - Failed to remove '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-u7bgm8or/storage' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: 'storage'\n",
      "2025-05-19 12:50:46,008 - distributed.diskutils - ERROR - Failed to remove '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-u7bgm8or' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-u7bgm8or'\n",
      "2025-05-19 12:50:46,009 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37819\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53554 remote=tcp://127.0.0.1:37819>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:46,009 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37819\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:33410 remote=tcp://127.0.0.1:37819>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:46,008 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37819\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47636 remote=tcp://127.0.0.1:37819>: Stream is closed\n",
      "2025-05-19 12:50:46,975 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45947\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50932 remote=tcp://127.0.0.1:45947>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:50,947 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41269\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:58446 remote=tcp://127.0.0.1:41269>: Stream is closed\n",
      "2025-05-19 12:50:51,150 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40651\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:39336 remote=tcp://127.0.0.1:40651>: Stream is closed\n",
      "2025-05-19 12:50:51,149 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40651\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53490 remote=tcp://127.0.0.1:40651>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:54,223 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.34 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:54,573 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 4.57 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:55,498 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.25 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:59,047 - distributed.worker.memory - WARNING - Worker is at 46% memory usage. Resuming worker. Process memory: 2.42 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:59,291 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 4.43 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:59,448 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33225\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:38700 remote=tcp://127.0.0.1:33225>: Stream is closed\n",
      "2025-05-19 12:50:59,449 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33225\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:48376 remote=tcp://127.0.0.1:33225>: Stream is closed\n",
      "2025-05-19 12:50:59,582 - distributed.worker.memory - WARNING - Worker is at 46% memory usage. Resuming worker. Process memory: 2.41 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:01,647 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46323\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:38696 remote=tcp://127.0.0.1:46323>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:51:01,980 - distributed.worker.memory - WARNING - Worker is at 90% memory usage. Pausing worker.  Process memory: 4.69 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:02,070 - distributed.worker.memory - WARNING - Worker is at 91% memory usage. Pausing worker.  Process memory: 4.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:02,472 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 3.26 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:02,876 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 3.08 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:03,249 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.26 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-3960: Attempted to run task ('sum-sum-aggregate-getitem-getitem-862b32cb709147f01cc9d606b8f97b49', 0, 0, 0, 5) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:46323. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-10080 already complete.\n",
      "t0-9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:51:03,814 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.95 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:03,886 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:03,897 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.78 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:04,159 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.30 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:05,028 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.79 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:54:16,913 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38563\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:33664 remote=tcp://127.0.0.1:38563>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:54:17,909 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.20 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,071 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.19 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,336 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 3.12 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,337 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,922 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 3.11 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,942 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:21,041 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.35 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:21,278 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:22,495 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36913\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:36614 remote=tcp://127.0.0.1:36913>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-9720: Attempted to run task ('sum-sum-aggregate-getitem-getitem-c21a0a667c39bd0e2a34be7c0b73ab64', 0, 0, 0, 12) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:44399. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-5040 already complete.\n",
      "t0-6120\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:56:06,489 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.27 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:06,570 - distributed.diskutils - ERROR - Failed to remove '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-d1_305cg/storage' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: 'storage'\n",
      "2025-05-19 12:56:06,570 - distributed.diskutils - ERROR - Failed to remove '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-d1_305cg' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-d1_305cg'\n",
      "2025-05-19 12:56:11,231 - distributed.worker.memory - WARNING - Worker is at 90% memory usage. Pausing worker.  Process memory: 4.69 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:12,018 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 3.18 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:12,547 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.18 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:14,946 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42111\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:49182 remote=tcp://127.0.0.1:42111>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:56:14,947 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42111\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:49196 remote=tcp://127.0.0.1:42111>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:56:15,128 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35213\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils.py\", line 1957, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152ae5999390>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1535, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-05-19 12:56:19,998 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45639\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:35732 remote=tcp://127.0.0.1:45639>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:56:20,357 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.68 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-6120: Attempted to run task ('sum-sum-aggregate-getitem-getitem-4af4bcd28efe098d64066b8dcc7375bc', 0, 0, 0, 3) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45675. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4216 already complete.\n",
      "t0-10800 already complete.\n",
      "t0-9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:56:20,760 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:20,858 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.66 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:21,145 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:21,179 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:21,256 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:21,261 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:59:31,540 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43305\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:49308 remote=tcp://127.0.0.1:43305>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:59:35,747 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.19 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:59:35,986 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35115\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37182 remote=tcp://127.0.0.1:35115>: Stream is closed\n",
      "2025-05-19 12:59:39,006 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:59:39,279 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.66 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-9000: Attempted to run task ('sum-sum-aggregate-getitem-getitem-753bf4c2ff14e95838239e489b31cdf9', 0, 0, 0, 7) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:42147. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7920 already complete.\n",
      "t0-7200 already complete.\n",
      "t0-22200 already complete.\n",
      "t0-10440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:59:39,809 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.21 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:59:39,878 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:01:21,787 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33467\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:45126 remote=tcp://127.0.0.1:33467>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:01:27,512 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44537\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:52926 remote=tcp://127.0.0.1:44537>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:01:30,388 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.79 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-10440: Attempted to run task ('sum-sum-aggregate-getitem-getitem-835911b6f7d0d7ee2ab81e6e018cd8a1', 0, 0, 0, 5) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:41175. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6480 already complete.\n",
      "t0-5760 already complete.\n",
      "t0-4680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:01:30,610 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.65 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:30,665 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.25 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:30,741 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.77 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:30,790 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,050 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,110 - distributed.worker.memory - WARNING - Worker is at 20% memory usage. Resuming worker. Process memory: 1.07 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,193 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.34 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,242 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,673 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.83 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:01:32,035 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.18 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:32,073 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:04:32,892 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41039\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:35536 remote=tcp://127.0.0.1:41039>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:36,028 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46095\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37206 remote=tcp://127.0.0.1:46095>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:43,321 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42333\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:47078 remote=tcp://127.0.0.1:42333>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:43,321 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42333\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:47082 remote=tcp://127.0.0.1:42333>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:43,660 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39287\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42314 remote=tcp://127.0.0.1:39287>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:48,907 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:04:48,923 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.79 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-4680: Attempted to run task ('sum-sum-aggregate-getitem-getitem-14219f83c476d5354ad4b70c43e5e9be', 0, 0, 0, 12) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:44817. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:04:49,178 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:04:49,307 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:04:49,432 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:04:49,523 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:06:34,115 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42455\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:33394 remote=tcp://127.0.0.1:42455>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:06:38,131 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:34689\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:56550 remote=tcp://127.0.0.1:34689>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-6840: Attempted to run task ('sum-sum-aggregate-getitem-getitem-170264c4273018b00c6006f29e93cd50', 0, 0, 0, 7) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:33275. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4320 already complete.\n",
      "t0-5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:06:46,112 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.97 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,188 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.84 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,226 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,558 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.32 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,609 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,732 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.34 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,752 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 4.46 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,787 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.77 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,827 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:07:49,576 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47086 remote=tcp://127.0.0.1:42179>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:07:49,576 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47060 remote=tcp://127.0.0.1:42179>: Stream is closed\n",
      "2025-05-19 13:07:49,577 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47080 remote=tcp://127.0.0.1:42179>: Stream is closed\n",
      "2025-05-19 13:07:49,577 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:47098 remote=tcp://127.0.0.1:42179>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:07:49,577 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47076 remote=tcp://127.0.0.1:42179>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-5400: Attempted to run task ('sum-sum-aggregate-getitem-getitem-1a015dc65063753eb6bf2363d6d2a0a5', 0, 0, 0, 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:36485. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-9360 already complete.\n",
      "t0-8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:08:04,113 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.83 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:08:04,537 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 4.38 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:08:04,609 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:09:33,549 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37735\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:56088 remote=tcp://127.0.0.1:37735>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:09:35,436 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:09:44,947 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-8280: Attempted to run task ('sum-sum-aggregate-getitem-getitem-bb01fd8c6d65850bdb8896ad0081cbe7', 0, 0, 0, 4) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:39927. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:09:45,377 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.21 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:09:45,446 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:10:36,002 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45707\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils.py\", line 1957, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148128076800>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1535, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-05-19 13:10:38,985 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41879\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:54474 remote=tcp://127.0.0.1:41879>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:10:39,830 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.82 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-7560: Attempted to run task ('sum-sum-aggregate-getitem-getitem-4f3a828dc2042670a1d7c47e35ae03d3', 0, 0, 0, 1) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45555. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-8640 already complete.\n",
      "t0-4416 already complete.\n",
      "Done with  full-40\n",
      "Starting beamless-40\n",
      "t0-3960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:10:40,295 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 4.41 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:10:40,331 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:10:40,524 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:10:41,121 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:10:41,124 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:11:45,649 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45583\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:45530 remote=tcp://127.0.0.1:45583>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:11:45,649 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45583\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:45518 remote=tcp://127.0.0.1:45583>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:11:55,676 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38937\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:55928 remote=tcp://127.0.0.1:38937>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:11:55,676 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38937\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:55926 remote=tcp://127.0.0.1:38937>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:11:55,676 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38937\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:55912 remote=tcp://127.0.0.1:38937>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:12:01,393 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.77 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-3960: Attempted to run task ('sum-sum-aggregate-getitem-getitem-629d41d228a0464c12f02845ae05a77f', 0, 0, 0, 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:46405. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-10080 already complete.\n",
      "t0-9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:12:01,636 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.87 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:01,912 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.34 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:01,956 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.84 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:01,993 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,037 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,124 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.27 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,134 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,207 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.21 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,250 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 4.38 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,253 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,337 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:15:03,704 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33879\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:38278 remote=tcp://127.0.0.1:33879>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:15:03,704 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33879\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:38274 remote=tcp://127.0.0.1:33879>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:15:08,741 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38647\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:58270 remote=tcp://127.0.0.1:38647>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-9720: Attempted to run task ('sum-sum-aggregate-getitem-getitem-6297ab17a3b92c169fa02a03b388139a', 0, 0, 0, 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:41053. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-5040 already complete.\n",
      "t0-6120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:15:08,812 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.69 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:17:03,100 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35239\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils.py\", line 1957, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x145dbf2eebf0>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1535, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-05-19 13:17:05,740 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-6120: Attempted to run task ('sum-sum-aggregate-getitem-getitem-9ae6ead6ef416386c84e42abc7f019c2', 0, 0, 0, 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:43223. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4216 already complete.\n",
      "t0-10800 already complete.\n",
      "t0-9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:17:06,133 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.71 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,181 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.69 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,265 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 4.43 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,338 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,375 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.67 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,945 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,975 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing beamless-40 at time t0-9000: Attempted to run task ('sum-sum-aggregate-getitem-getitem-7e34bb5b63bd9345a16c988bb30a5fdf', 0, 0, 0, 3) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:34701. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7920 already complete.\n",
      "t0-7200 already complete.\n",
      "t0-22200\n",
      "Error processing beamless-40 at time t0-22200: no files to open\n",
      "t0-10440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:18:24,196 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.91 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,308 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.67 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,383 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,508 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.32 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,590 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.26 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,593 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,708 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,768 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,924 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.26 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,983 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:25,156 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.21 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:25,167 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing beamless-40 at time t0-10440: Attempted to run task ('sum-sum-aggregate-getitem-getitem-51ccdd96af9f230bbff57a99f594b3d4', 0, 0, 0, 3) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:37685. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6480 already complete.\n",
      "t0-5760 already complete.\n",
      "t0-4680\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:22:19,192 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43391\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42478 remote=tcp://127.0.0.1:43391>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-4680: Attempted to run task ('sum-sum-aggregate-getitem-getitem-943b09550ed5020bd65b08492dfc11a7', 0, 0, 0, 5) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:37923. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:22:27,221 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,276 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.77 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,420 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.68 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,804 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 4.40 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,875 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,887 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.27 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,913 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.35 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,919 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,920 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:25:16,373 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44903\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44744 remote=tcp://127.0.0.1:44903>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:16,373 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44903\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44724 remote=tcp://127.0.0.1:44903>: Stream is closed\n",
      "2025-05-19 13:25:16,373 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44903\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44734 remote=tcp://127.0.0.1:44903>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:16,373 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44903\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44738 remote=tcp://127.0.0.1:44903>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:19,039 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40109\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:55664 remote=tcp://127.0.0.1:40109>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:20,746 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33065\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:60442 remote=tcp://127.0.0.1:33065>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:20,990 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40197\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:59788 remote=tcp://127.0.0.1:40197>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-6840: Attempted to run task ('sum-sum-aggregate-getitem-getitem-c4413a1a9e2f7b72653924bf071fc1ac', 0, 0, 0, 11) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:40197. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4320 already complete.\n",
      "t0-5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:25:21,446 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.66 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:21,479 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.81 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:21,909 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:21,920 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:21,986 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.29 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,080 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,121 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.32 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,146 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:25:22,558 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.68 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,819 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.24 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,958 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:28:03,904 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45609\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42122 remote=tcp://127.0.0.1:45609>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:28:03,904 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45609\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42132 remote=tcp://127.0.0.1:45609>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:28:17,288 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.84 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-5400: Attempted to run task ('sum-sum-aggregate-getitem-getitem-3dad12905b35ca4668aafb5e727ee377', 0, 0, 0, 5) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45701. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-9360 already complete.\n",
      "t0-8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:28:17,611 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.65 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:28:17,825 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.67 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:28:17,841 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 4.43 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:28:17,887 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:29:54,943 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44943\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:59032 remote=tcp://127.0.0.1:44943>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:30:05,021 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41563\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:38798 remote=tcp://127.0.0.1:41563>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:30:08,096 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-8280: Attempted to run task ('sum-sum-aggregate-getitem-getitem-261695895cc976ca757ce3e7b9189a00', 0, 0, 0, 10) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:35709. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:30:08,305 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:30:08,555 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:30:08,601 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:30:08,688 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.19 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:30:08,694 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:32:42,644 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45485\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44244 remote=tcp://127.0.0.1:45485>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:32:42,645 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45485\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44234 remote=tcp://127.0.0.1:45485>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:32:44,828 - distributed.worker.memory - WARNING - Worker is at 92% memory usage. Pausing worker.  Process memory: 4.82 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:32:45,797 - distributed.worker.memory - WARNING - Worker is at 50% memory usage. Resuming worker. Process memory: 2.65 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-7560: Attempted to run task ('sum-sum-aggregate-getitem-getitem-e7b50aa701f4ddcf434551e57119eed1', 0, 0, 0, 9) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45339. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-8640 already complete.\n",
      "t0-4416 already complete.\n",
      "Done with  beamless-40\n",
      "Starting smooth-40\n",
      "t0-3960\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing smooth-40 at time t0-3960: Attempted to run task ('sum-sum-aggregate-getitem-getitem-1e9598d4bd36435d1622eb28908ba1b7', 0, 0, 0, 4) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:40305. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-10080 already complete.\n",
      "t0-9720\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:35:08,453 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-9720: Attempted to run task ('sum-sum-aggregate-getitem-getitem-82cb470dcab40375017b573447a6fea4', 0, 0, 0, 2) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:37017. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-5040\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:35:54,858 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:34335\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:40434 remote=tcp://127.0.0.1:34335>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-5040: Attempted to run task ('sum-sum-aggregate-getitem-getitem-a5f7f29bd3cf6bd8e72b95803ef2973f', 0, 0, 0, 8) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:37895. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6120\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:37:19,809 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36677\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:46336 remote=tcp://127.0.0.1:36677>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:37:27,143 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38449\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53176 remote=tcp://127.0.0.1:38449>: Stream is closed\n",
      "2025-05-19 13:37:36,120 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-6120: Attempted to run task ('sum-sum-aggregate-getitem-getitem-103c9c09271e7aaa8a287b2da5d363a8', 0, 0, 0, 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:46633. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4216 already complete.\n",
      "t0-10800 already complete.\n",
      "t0-9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:37:36,452 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.91 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:37:36,489 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.19 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:37:36,518 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.71 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:37:36,604 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.22 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:37:36,751 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing smooth-40 at time t0-9000: Attempted to run task ('sum-sum-aggregate-getitem-getitem-f9b873c86fb1ee51efaf192dd32ed9c8', 0, 0, 0, 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:33039. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7920 already complete.\n",
      "t0-7200 already complete.\n",
      "t0-22200\n",
      "Error processing smooth-40 at time t0-22200: no files to open\n",
      "t0-10440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:39:31,644 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:39:32,415 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:39:32,787 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:39:32,815 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing smooth-40 at time t0-10440: Attempted to run task ('sum-sum-aggregate-getitem-getitem-458fddef5f4dd45cae0c972ef93ca987', 0, 0, 0, 2) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45495. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6480 already complete.\n",
      "t0-5760\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:41:53,115 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44237\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:33836 remote=tcp://127.0.0.1:44237>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-5760: Attempted to run task ('sum-sum-aggregate-getitem-getitem-98c72329c6faaf41ff4717ee51c0390b', 0, 0, 0, 2) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:40325. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:42:08,491 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.71 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:42:09,025 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:42:09,087 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:44:58,285 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33397\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:57388 remote=tcp://127.0.0.1:33397>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-4680: Attempted to run task ('sum-sum-aggregate-getitem-getitem-718027bb74e37b315f481a6472623327', 0, 0, 0, 7) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:41791. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6840\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:46:00,806 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41039\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:39246 remote=tcp://127.0.0.1:41039>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:46:01,067 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38877\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:58682 remote=tcp://127.0.0.1:38877>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:46:09,329 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.66 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-6840: Attempted to run task ('sum-sum-aggregate-getitem-getitem-e4f0dea7dc72d3cc34079265f0eeecdd', 0, 0, 0, 3) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:42307. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4320 already complete.\n",
      "t0-5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:46:09,895 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.24 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:46:09,926 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:47:08,000 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33599\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:37802 remote=tcp://127.0.0.1:33599>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:47:11,002 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38583\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:51060 remote=tcp://127.0.0.1:38583>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:49:33,733 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('sum-sum-aggregate-getitem-getitem-bf494ad25202c156026f26d0ce635219', 0, 0, 0, 0))\" coro=<Worker.execute() done, defined at /g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2025-05-19 13:49:33,974 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:57958 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,975 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56538 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,977 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:50398 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,977 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56606 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,998 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:35756 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,996 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:50484 remote=tcp://127.0.0.1:38065>: Stream is closed\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "expts = [\"full-10\",\"beamless-10\",\"smooth-10\",\"beamless-20\",\"smooth-20\",\"full-20\",\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "# expts = [\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "t0_20th = 22000\n",
    "t0_40th = 4216\n",
    "for expt in expts:\n",
    "    print(f\"Starting {expt}\" )\n",
    "\n",
    "    times = os.listdir(f\"/g/data/nm03/ab8992/postprocessed/{expt}/bandpassed/\")\n",
    "    for t0 in times:\n",
    "        outputdir = f\"/scratch/nm03/ab8992/april-manytimes/{expt}/{t0}\"\n",
    "        # CHeck how many files are in the directory\n",
    "        if os.path.exists(outputdir) and len(os.listdir(outputdir)) == 18:\n",
    "            print(f\"{t0} already complete.\")\n",
    "            continue\n",
    "        print(t0)\n",
    "        tmpstorage = os.getenv('PBS_JOBFS') + \"/tmpstorage\"\n",
    "        # tmpstorage = \"/scratch/nm03/ab8992/april-tmpstorage/tmpstorage/\"+expt\n",
    "        if os.path.exists(tmpstorage):\n",
    "            shutil.rmtree(tmpstorage)\n",
    "        if os.path.exists(outputdir):\n",
    "            shutil.rmtree(outputdir)\n",
    "        os.makedirs(outputdir)\n",
    "        os.makedirs(tmpstorage)\n",
    "       \n",
    "        try:\n",
    "            save_temporary(expt, t0.split(\"-\")[-1], outputdir)\n",
    "            save_modal(outputdir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {expt} at time {t0}: {e}\")\n",
    "    print(\"Done with \",expt)\n",
    "    # clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = xr.open_mfdataset(f\"/g/data/nm03/ab8992/postprocessed/{expt}/bandpassed/t0-{12023}/Filtered*.nc\",decode_times = False)\n",
    "vmodes= xr.open_dataset(f\"/g/data/nm03/ab8992/postprocessed/{expt}/vertical_eigenfunctions/vmode-t0-{12258}.nc\",decode_times = False,chunks = {\"mode\":1}).isel(zl = slice(0,96))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.chunks[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 162GB\n",
       "Dimensions:  (zl: 96, time: 233, yb: 401, xb: 1501)\n",
       "Coordinates:\n",
       "  * zl       (zl) float64 768B 2.704 8.126 13.58 ... 4.926e+03 5.029e+03\n",
       "  * time     (time) float64 2kB 1.196e+04 1.196e+04 ... 1.219e+04 1.219e+04\n",
       "  * xb       (xb) float64 12kB -0.0 1.0 2.0 3.0 ... 1.498e+03 1.499e+03 1.5e+03\n",
       "  * yb       (yb) float64 3kB -200.0 -199.0 -198.0 -197.0 ... 198.0 199.0 200.0\n",
       "Data variables:\n",
       "    u        (zl, time, yb, xb) float32 54GB dask.array&lt;chunksize=(96, 17, 401, 1501), meta=np.ndarray&gt;\n",
       "    v        (zl, time, yb, xb) float32 54GB dask.array&lt;chunksize=(96, 17, 401, 1501), meta=np.ndarray&gt;\n",
       "    rho      (zl, time, yb, xb) float32 54GB dask.array&lt;chunksize=(96, 17, 401, 1501), meta=np.ndarray&gt;\n",
       "Attributes:\n",
       "    long_name:  filtered velocity data</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-fc93a02b-0948-4a42-a733-ee1132cc3955' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-fc93a02b-0948-4a42-a733-ee1132cc3955' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>zl</span>: 96</li><li><span class='xr-has-index'>time</span>: 233</li><li><span class='xr-has-index'>yb</span>: 401</li><li><span class='xr-has-index'>xb</span>: 1501</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-a1a87372-9d92-40d4-b40d-5f5137dbf979' class='xr-section-summary-in' type='checkbox'  checked><label for='section-a1a87372-9d92-40d4-b40d-5f5137dbf979' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>zl</span></div><div class='xr-var-dims'>(zl)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>2.704 8.126 ... 4.926e+03 5.029e+03</div><input id='attrs-a0f40c20-c4ea-4d54-9f4a-5f1fd7352ad1' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-a0f40c20-c4ea-4d54-9f4a-5f1fd7352ad1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-85f63572-7827-497c-a851-a637585c8894' class='xr-var-data-in' type='checkbox'><label for='data-85f63572-7827-497c-a851-a637585c8894' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>meters</dd><dt><span>long_name :</span></dt><dd>Depth at cell center</dd><dt><span>axis :</span></dt><dd>Z</dd><dt><span>positive :</span></dt><dd>down</dd><dt><span>edges :</span></dt><dd>z_i</dd></dl></div><div class='xr-var-data'><pre>array([2.703753e+00, 8.125880e+00, 1.357918e+01, 1.906775e+01, 2.459623e+01,\n",
       "       3.016989e+01, 3.579464e+01, 4.147720e+01, 4.722515e+01, 5.304703e+01,\n",
       "       5.895252e+01, 6.495250e+01, 7.105930e+01, 7.728676e+01, 8.365052e+01,\n",
       "       9.016818e+01, 9.685954e+01, 1.037469e+02, 1.108551e+02, 1.182124e+02,\n",
       "       1.258502e+02, 1.338036e+02, 1.421120e+02, 1.508192e+02, 1.599741e+02,\n",
       "       1.696308e+02, 1.798495e+02, 1.906965e+02, 2.022449e+02, 2.145749e+02,\n",
       "       2.277741e+02, 2.419380e+02, 2.571697e+02, 2.735803e+02, 2.912884e+02,\n",
       "       3.104201e+02, 3.311080e+02, 3.534903e+02, 3.777098e+02, 4.039118e+02,\n",
       "       4.322427e+02, 4.628474e+02, 4.958665e+02, 5.314341e+02, 5.696743e+02,\n",
       "       6.106986e+02, 6.546025e+02, 7.014638e+02, 7.513394e+02, 8.042643e+02,\n",
       "       8.602503e+02, 9.192855e+02, 9.813352e+02, 1.046342e+03, 1.114229e+03,\n",
       "       1.184899e+03, 1.258243e+03, 1.334134e+03, 1.412441e+03, 1.493021e+03,\n",
       "       1.575730e+03, 1.660421e+03, 1.746950e+03, 1.835173e+03, 1.924952e+03,\n",
       "       2.016155e+03, 2.108655e+03, 2.202334e+03, 2.297081e+03, 2.392793e+03,\n",
       "       2.489374e+03, 2.586736e+03, 2.684800e+03, 2.783492e+03, 2.882746e+03,\n",
       "       2.982502e+03, 3.082706e+03, 3.183309e+03, 3.284266e+03, 3.385539e+03,\n",
       "       3.487093e+03, 3.588895e+03, 3.690919e+03, 3.793138e+03, 3.895532e+03,\n",
       "       3.998079e+03, 4.100762e+03, 4.203566e+03, 4.306477e+03, 4.409483e+03,\n",
       "       4.512572e+03, 4.615735e+03, 4.718963e+03, 4.822249e+03, 4.925586e+03,\n",
       "       5.028969e+03])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>1.196e+04 1.196e+04 ... 1.219e+04</div><input id='attrs-61379e61-6991-4622-9b68-df228bae9804' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-61379e61-6991-4622-9b68-df228bae9804' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d089cf6f-abff-4fcc-80ca-1ebc3f72730f' class='xr-var-data-in' type='checkbox'><label for='data-d089cf6f-abff-4fcc-80ca-1ebc3f72730f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>time</dd><dt><span>calendar :</span></dt><dd>julian</dd><dt><span>axis :</span></dt><dd>T</dd><dt><span>calendar_type :</span></dt><dd>JULIAN</dd><dt><span>units :</span></dt><dd>hours since 2010-01-01 00:00:00</dd></dl></div><div class='xr-var-data'><pre>array([11955., 11956., 11957., ..., 12185., 12186., 12187.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>xb</span></div><div class='xr-var-dims'>(xb)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-0.0 1.0 2.0 ... 1.499e+03 1.5e+03</div><input id='attrs-f8cba173-f9a2-4af3-8c66-cac8b68a6972' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-f8cba173-f9a2-4af3-8c66-cac8b68a6972' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-898b42d6-435c-417a-8445-6bce6c61bcfb' class='xr-var-data-in' type='checkbox'><label for='data-898b42d6-435c-417a-8445-6bce6c61bcfb' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>km</dd><dt><span>long_name :</span></dt><dd>Distance along beam from Tasmania towards Macquarie Ridge</dd></dl></div><div class='xr-var-data'><pre>array([-0.000e+00,  1.000e+00,  2.000e+00, ...,  1.498e+03,  1.499e+03,\n",
       "        1.500e+03])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>yb</span></div><div class='xr-var-dims'>(yb)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-200.0 -199.0 ... 199.0 200.0</div><input id='attrs-9e9ba69d-bf05-4cdc-8345-69d0f8b22c2a' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-9e9ba69d-bf05-4cdc-8345-69d0f8b22c2a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-26cdda38-14ab-4182-a9d6-7a0bf9e79549' class='xr-var-data-in' type='checkbox'><label for='data-26cdda38-14ab-4182-a9d6-7a0bf9e79549' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>km</dd><dt><span>long_name :</span></dt><dd>Distance perpendicular to beam referened from beam centre</dd></dl></div><div class='xr-var-data'><pre>array([-200., -199., -198., ...,  198.,  199.,  200.])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-b0b8f412-3ee7-4811-9c2e-93f9548889f4' class='xr-section-summary-in' type='checkbox'  checked><label for='section-b0b8f412-3ee7-4811-9c2e-93f9548889f4' class='xr-section-summary' >Data variables: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>u</span></div><div class='xr-var-dims'>(zl, time, yb, xb)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(96, 17, 401, 1501), meta=np.ndarray&gt;</div><input id='attrs-424657e3-6505-4fee-ac18-8242c7c690e0' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-424657e3-6505-4fee-ac18-8242c7c690e0' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e8f83973-c1df-4c5f-a92f-afd6700f4fa4' class='xr-var-data-in' type='checkbox'><label for='data-e8f83973-c1df-4c5f-a92f-afd6700f4fa4' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 50.15 GiB </td>\n",
       "                        <td> 3.66 GiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (96, 233, 401, 1501) </td>\n",
       "                        <td> (96, 17, 401, 1501) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 14 chunks in 29 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"404\" height=\"117\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 35.99853926740522,0.0 35.99853926740522,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"17.999270\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >96</text>\n",
       "  <text x=\"55.998539\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,55.998539,12.706308)\">1</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"129\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"105\" y1=\"43\" x2=\"129\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"105\" y2=\"43\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"106\" y1=\"1\" x2=\"106\" y2=\"44\" />\n",
       "  <line x1=\"108\" y1=\"3\" x2=\"108\" y2=\"46\" />\n",
       "  <line x1=\"110\" y1=\"5\" x2=\"110\" y2=\"48\" />\n",
       "  <line x1=\"112\" y1=\"7\" x2=\"112\" y2=\"50\" />\n",
       "  <line x1=\"113\" y1=\"8\" x2=\"113\" y2=\"51\" />\n",
       "  <line x1=\"115\" y1=\"10\" x2=\"115\" y2=\"53\" />\n",
       "  <line x1=\"117\" y1=\"12\" x2=\"117\" y2=\"55\" />\n",
       "  <line x1=\"119\" y1=\"14\" x2=\"119\" y2=\"57\" />\n",
       "  <line x1=\"120\" y1=\"15\" x2=\"120\" y2=\"58\" />\n",
       "  <line x1=\"122\" y1=\"17\" x2=\"122\" y2=\"60\" />\n",
       "  <line x1=\"124\" y1=\"19\" x2=\"124\" y2=\"62\" />\n",
       "  <line x1=\"126\" y1=\"21\" x2=\"126\" y2=\"64\" />\n",
       "  <line x1=\"127\" y1=\"22\" x2=\"127\" y2=\"65\" />\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"129\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"105.0,0.0 129.04156432464998,24.041564324649983 129.04156432464998,67.18382631011156 105.0,43.14226198546158\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"225\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"106\" y1=\"1\" x2=\"226\" y2=\"1\" />\n",
       "  <line x1=\"108\" y1=\"3\" x2=\"228\" y2=\"3\" />\n",
       "  <line x1=\"110\" y1=\"5\" x2=\"230\" y2=\"5\" />\n",
       "  <line x1=\"112\" y1=\"7\" x2=\"232\" y2=\"7\" />\n",
       "  <line x1=\"113\" y1=\"8\" x2=\"233\" y2=\"8\" />\n",
       "  <line x1=\"115\" y1=\"10\" x2=\"235\" y2=\"10\" />\n",
       "  <line x1=\"117\" y1=\"12\" x2=\"237\" y2=\"12\" />\n",
       "  <line x1=\"119\" y1=\"14\" x2=\"239\" y2=\"14\" />\n",
       "  <line x1=\"120\" y1=\"15\" x2=\"240\" y2=\"15\" />\n",
       "  <line x1=\"122\" y1=\"17\" x2=\"242\" y2=\"17\" />\n",
       "  <line x1=\"124\" y1=\"19\" x2=\"244\" y2=\"19\" />\n",
       "  <line x1=\"126\" y1=\"21\" x2=\"246\" y2=\"21\" />\n",
       "  <line x1=\"127\" y1=\"22\" x2=\"247\" y2=\"22\" />\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"249\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"129\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"225\" y1=\"0\" x2=\"249\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"105.0,0.0 225.0,0.0 249.04156432464998,24.041564324649983 129.04156432464998,24.041564324649983\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"249\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"129\" y1=\"67\" x2=\"249\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"129\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"249\" y1=\"24\" x2=\"249\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"129.04156432464998,24.041564324649983 249.04156432464998,24.041564324649983 249.04156432464998,67.18382631011156 129.04156432464998,67.18382631011156\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"189.041564\" y=\"87.183826\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1501</text>\n",
       "  <text x=\"269.041564\" y=\"45.612695\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,269.041564,45.612695)\">401</text>\n",
       "  <text x=\"107.020782\" y=\"75.163044\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,107.020782,75.163044)\">233</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table></div></li><li class='xr-var-item'><div class='xr-var-name'><span>v</span></div><div class='xr-var-dims'>(zl, time, yb, xb)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(96, 17, 401, 1501), meta=np.ndarray&gt;</div><input id='attrs-b0d60e5c-a3db-45d4-9d36-0eb313fd0aa9' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-b0d60e5c-a3db-45d4-9d36-0eb313fd0aa9' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f2405a9a-5cba-4f54-959a-8dce3934f774' class='xr-var-data-in' type='checkbox'><label for='data-f2405a9a-5cba-4f54-959a-8dce3934f774' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 50.15 GiB </td>\n",
       "                        <td> 3.66 GiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (96, 233, 401, 1501) </td>\n",
       "                        <td> (96, 17, 401, 1501) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 14 chunks in 29 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"404\" height=\"117\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 35.99853926740522,0.0 35.99853926740522,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"17.999270\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >96</text>\n",
       "  <text x=\"55.998539\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,55.998539,12.706308)\">1</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"129\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"105\" y1=\"43\" x2=\"129\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"105\" y2=\"43\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"106\" y1=\"1\" x2=\"106\" y2=\"44\" />\n",
       "  <line x1=\"108\" y1=\"3\" x2=\"108\" y2=\"46\" />\n",
       "  <line x1=\"110\" y1=\"5\" x2=\"110\" y2=\"48\" />\n",
       "  <line x1=\"112\" y1=\"7\" x2=\"112\" y2=\"50\" />\n",
       "  <line x1=\"113\" y1=\"8\" x2=\"113\" y2=\"51\" />\n",
       "  <line x1=\"115\" y1=\"10\" x2=\"115\" y2=\"53\" />\n",
       "  <line x1=\"117\" y1=\"12\" x2=\"117\" y2=\"55\" />\n",
       "  <line x1=\"119\" y1=\"14\" x2=\"119\" y2=\"57\" />\n",
       "  <line x1=\"120\" y1=\"15\" x2=\"120\" y2=\"58\" />\n",
       "  <line x1=\"122\" y1=\"17\" x2=\"122\" y2=\"60\" />\n",
       "  <line x1=\"124\" y1=\"19\" x2=\"124\" y2=\"62\" />\n",
       "  <line x1=\"126\" y1=\"21\" x2=\"126\" y2=\"64\" />\n",
       "  <line x1=\"127\" y1=\"22\" x2=\"127\" y2=\"65\" />\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"129\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"105.0,0.0 129.04156432464998,24.041564324649983 129.04156432464998,67.18382631011156 105.0,43.14226198546158\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"225\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"106\" y1=\"1\" x2=\"226\" y2=\"1\" />\n",
       "  <line x1=\"108\" y1=\"3\" x2=\"228\" y2=\"3\" />\n",
       "  <line x1=\"110\" y1=\"5\" x2=\"230\" y2=\"5\" />\n",
       "  <line x1=\"112\" y1=\"7\" x2=\"232\" y2=\"7\" />\n",
       "  <line x1=\"113\" y1=\"8\" x2=\"233\" y2=\"8\" />\n",
       "  <line x1=\"115\" y1=\"10\" x2=\"235\" y2=\"10\" />\n",
       "  <line x1=\"117\" y1=\"12\" x2=\"237\" y2=\"12\" />\n",
       "  <line x1=\"119\" y1=\"14\" x2=\"239\" y2=\"14\" />\n",
       "  <line x1=\"120\" y1=\"15\" x2=\"240\" y2=\"15\" />\n",
       "  <line x1=\"122\" y1=\"17\" x2=\"242\" y2=\"17\" />\n",
       "  <line x1=\"124\" y1=\"19\" x2=\"244\" y2=\"19\" />\n",
       "  <line x1=\"126\" y1=\"21\" x2=\"246\" y2=\"21\" />\n",
       "  <line x1=\"127\" y1=\"22\" x2=\"247\" y2=\"22\" />\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"249\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"129\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"225\" y1=\"0\" x2=\"249\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"105.0,0.0 225.0,0.0 249.04156432464998,24.041564324649983 129.04156432464998,24.041564324649983\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"249\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"129\" y1=\"67\" x2=\"249\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"129\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"249\" y1=\"24\" x2=\"249\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"129.04156432464998,24.041564324649983 249.04156432464998,24.041564324649983 249.04156432464998,67.18382631011156 129.04156432464998,67.18382631011156\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"189.041564\" y=\"87.183826\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1501</text>\n",
       "  <text x=\"269.041564\" y=\"45.612695\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,269.041564,45.612695)\">401</text>\n",
       "  <text x=\"107.020782\" y=\"75.163044\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,107.020782,75.163044)\">233</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table></div></li><li class='xr-var-item'><div class='xr-var-name'><span>rho</span></div><div class='xr-var-dims'>(zl, time, yb, xb)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(96, 17, 401, 1501), meta=np.ndarray&gt;</div><input id='attrs-9436fe25-989d-4847-b6b2-b3c2399750eb' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-9436fe25-989d-4847-b6b2-b3c2399750eb' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c8cc0b64-1fa7-4e69-87f7-6f0b539d49b8' class='xr-var-data-in' type='checkbox'><label for='data-c8cc0b64-1fa7-4e69-87f7-6f0b539d49b8' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 50.15 GiB </td>\n",
       "                        <td> 3.66 GiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (96, 233, 401, 1501) </td>\n",
       "                        <td> (96, 17, 401, 1501) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 14 chunks in 29 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"404\" height=\"117\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"35\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 35.99853926740522,0.0 35.99853926740522,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"17.999270\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >96</text>\n",
       "  <text x=\"55.998539\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,55.998539,12.706308)\">1</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"129\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"105\" y1=\"43\" x2=\"129\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"105\" y2=\"43\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"106\" y1=\"1\" x2=\"106\" y2=\"44\" />\n",
       "  <line x1=\"108\" y1=\"3\" x2=\"108\" y2=\"46\" />\n",
       "  <line x1=\"110\" y1=\"5\" x2=\"110\" y2=\"48\" />\n",
       "  <line x1=\"112\" y1=\"7\" x2=\"112\" y2=\"50\" />\n",
       "  <line x1=\"113\" y1=\"8\" x2=\"113\" y2=\"51\" />\n",
       "  <line x1=\"115\" y1=\"10\" x2=\"115\" y2=\"53\" />\n",
       "  <line x1=\"117\" y1=\"12\" x2=\"117\" y2=\"55\" />\n",
       "  <line x1=\"119\" y1=\"14\" x2=\"119\" y2=\"57\" />\n",
       "  <line x1=\"120\" y1=\"15\" x2=\"120\" y2=\"58\" />\n",
       "  <line x1=\"122\" y1=\"17\" x2=\"122\" y2=\"60\" />\n",
       "  <line x1=\"124\" y1=\"19\" x2=\"124\" y2=\"62\" />\n",
       "  <line x1=\"126\" y1=\"21\" x2=\"126\" y2=\"64\" />\n",
       "  <line x1=\"127\" y1=\"22\" x2=\"127\" y2=\"65\" />\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"129\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"105.0,0.0 129.04156432464998,24.041564324649983 129.04156432464998,67.18382631011156 105.0,43.14226198546158\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"225\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"106\" y1=\"1\" x2=\"226\" y2=\"1\" />\n",
       "  <line x1=\"108\" y1=\"3\" x2=\"228\" y2=\"3\" />\n",
       "  <line x1=\"110\" y1=\"5\" x2=\"230\" y2=\"5\" />\n",
       "  <line x1=\"112\" y1=\"7\" x2=\"232\" y2=\"7\" />\n",
       "  <line x1=\"113\" y1=\"8\" x2=\"233\" y2=\"8\" />\n",
       "  <line x1=\"115\" y1=\"10\" x2=\"235\" y2=\"10\" />\n",
       "  <line x1=\"117\" y1=\"12\" x2=\"237\" y2=\"12\" />\n",
       "  <line x1=\"119\" y1=\"14\" x2=\"239\" y2=\"14\" />\n",
       "  <line x1=\"120\" y1=\"15\" x2=\"240\" y2=\"15\" />\n",
       "  <line x1=\"122\" y1=\"17\" x2=\"242\" y2=\"17\" />\n",
       "  <line x1=\"124\" y1=\"19\" x2=\"244\" y2=\"19\" />\n",
       "  <line x1=\"126\" y1=\"21\" x2=\"246\" y2=\"21\" />\n",
       "  <line x1=\"127\" y1=\"22\" x2=\"247\" y2=\"22\" />\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"249\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"129\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"225\" y1=\"0\" x2=\"249\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"105.0,0.0 225.0,0.0 249.04156432464998,24.041564324649983 129.04156432464998,24.041564324649983\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"249\" y2=\"24\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"129\" y1=\"67\" x2=\"249\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"129\" y1=\"24\" x2=\"129\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"249\" y1=\"24\" x2=\"249\" y2=\"67\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"129.04156432464998,24.041564324649983 249.04156432464998,24.041564324649983 249.04156432464998,67.18382631011156 129.04156432464998,67.18382631011156\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"189.041564\" y=\"87.183826\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1501</text>\n",
       "  <text x=\"269.041564\" y=\"45.612695\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,269.041564,45.612695)\">401</text>\n",
       "  <text x=\"107.020782\" y=\"75.163044\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,107.020782,75.163044)\">233</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table></div></li></ul></div></li><li class='xr-section-item'><input id='section-74357877-0c19-43c9-888e-53ddc5ebb461' class='xr-section-summary-in' type='checkbox'  ><label for='section-74357877-0c19-43c9-888e-53ddc5ebb461' class='xr-section-summary' >Indexes: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>zl</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-f4fbd681-617d-4d3a-b901-3e04cdb073de' class='xr-index-data-in' type='checkbox'/><label for='index-f4fbd681-617d-4d3a-b901-3e04cdb073de' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([ 2.703753010361301,   8.12588009790504,  13.57917597818933,\n",
       "         19.0677462815569, 24.596233863642556, 30.169888095518928,\n",
       "        35.79464280769775,   41.4772038888342, 47.225147632769605,\n",
       "        53.04703102197908, 58.952515229037026,  64.95250370685102,\n",
       "        71.05929631834977,  77.28676102065936,  83.65052465919959,\n",
       "        90.16818443278804,  96.85954154803169, 103.74685847280408,\n",
       "       110.85514100317826, 118.21244604991921, 125.85021559855585,\n",
       "       133.80363666481867, 142.11202621323997,  150.8192388847458,\n",
       "        159.9740939397846,  169.6308160172191, 179.84948209014027,\n",
       "       190.69646433279377, 202.24485548254768, 214.57485970389612,\n",
       "       227.77412800198698, 241.93801302094812, 257.16971381302045,\n",
       "        273.5802771990335, 291.28841910009595, 310.42012727064116,\n",
       "        331.1080068830699, 353.49033315796277,  377.7097814586537,\n",
       "         403.911815626739, 432.24273022650823, 462.84736177092253,\n",
       "       495.86650730936975,   531.434114639213,  569.6743347715629,\n",
       "        610.6985513861348,  654.6025206992952,  701.4637653176878,\n",
       "        751.3393646959448,  804.2642713175994,  860.2502557850537,\n",
       "        919.2855474959056,  981.3351939666219, 1046.3421157425705,\n",
       "       1114.2287902171076, 1184.8994611738667, 1258.2427449331324,\n",
       "       1334.1344904837943, 1412.4407500284888, 1493.0207265178283,\n",
       "        1575.729583438852,   1660.42102622727,  1746.949591041486,\n",
       "        1835.172602518166,   1924.95178543673, 2016.1545346247765,\n",
       "       2108.6548623278727, 2202.3340526249094, 2297.0810586950574,\n",
       "       2392.7926814860753, 2489.3735683538357,  2586.736068293191,\n",
       "        2684.799977139646,  2783.492202155834, 2882.7463711675086,\n",
       "       2982.5024072015785,  3082.706085619182, 3183.3085871598696,\n",
       "       3284.2660571827155, 3385.5391787231874, 3487.0927647655553,\n",
       "         3588.89537332429, 3690.9189474886266, 3793.1384814624917,\n",
       "       3895.5317127780127,  3998.078840228581,   4100.76226661538,\n",
       "        4203.566365092991,  4306.477267704286, 4409.4826745863365,\n",
       "        4512.571682286236,   4615.73462963141, 4718.9629596393825,\n",
       "        4822.249096016312,  4925.586332873544,  5028.968736380568],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;zl&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-04c008b0-5273-434f-9cae-ac5c20599ef0' class='xr-index-data-in' type='checkbox'/><label for='index-04c008b0-5273-434f-9cae-ac5c20599ef0' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([11955.0, 11956.0, 11957.0, 11958.0, 11959.0, 11960.0, 11961.0, 11962.0,\n",
       "       11963.0, 11964.0,\n",
       "       ...\n",
       "       12178.0, 12179.0, 12180.0, 12181.0, 12182.0, 12183.0, 12184.0, 12185.0,\n",
       "       12186.0, 12187.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;time&#x27;, length=233))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>xb</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-7d983659-4db2-4f84-92d2-d15151243f0d' class='xr-index-data-in' type='checkbox'/><label for='index-7d983659-4db2-4f84-92d2-d15151243f0d' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([  -0.0,    1.0,    2.0,    3.0,    4.0,    5.0,    6.0,    7.0,    8.0,\n",
       "          9.0,\n",
       "       ...\n",
       "       1491.0, 1492.0, 1493.0, 1494.0, 1495.0, 1496.0, 1497.0, 1498.0, 1499.0,\n",
       "       1500.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;xb&#x27;, length=1501))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>yb</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-e6c5293b-7392-488b-a43c-5120cce2acf6' class='xr-index-data-in' type='checkbox'/><label for='index-e6c5293b-7392-488b-a43c-5120cce2acf6' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-200.0, -199.0, -198.0, -197.0, -196.0, -195.0, -194.0, -193.0, -192.0,\n",
       "       -191.0,\n",
       "       ...\n",
       "        191.0,  192.0,  193.0,  194.0,  195.0,  196.0,  197.0,  198.0,  199.0,\n",
       "        200.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;yb&#x27;, length=401))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-44c89b07-2489-4d1a-a528-ead3c7273f20' class='xr-section-summary-in' type='checkbox'  checked><label for='section-44c89b07-2489-4d1a-a528-ead3c7273f20' class='xr-section-summary' >Attributes: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>filtered velocity data</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 162GB\n",
       "Dimensions:  (zl: 96, time: 233, yb: 401, xb: 1501)\n",
       "Coordinates:\n",
       "  * zl       (zl) float64 768B 2.704 8.126 13.58 ... 4.926e+03 5.029e+03\n",
       "  * time     (time) float64 2kB 1.196e+04 1.196e+04 ... 1.219e+04 1.219e+04\n",
       "  * xb       (xb) float64 12kB -0.0 1.0 2.0 3.0 ... 1.498e+03 1.499e+03 1.5e+03\n",
       "  * yb       (yb) float64 3kB -200.0 -199.0 -198.0 -197.0 ... 198.0 199.0 200.0\n",
       "Data variables:\n",
       "    u        (zl, time, yb, xb) float32 54GB dask.array<chunksize=(96, 17, 401, 1501), meta=np.ndarray>\n",
       "    v        (zl, time, yb, xb) float32 54GB dask.array<chunksize=(96, 17, 401, 1501), meta=np.ndarray>\n",
       "    rho      (zl, time, yb, xb) float32 54GB dask.array<chunksize=(96, 17, 401, 1501), meta=np.ndarray>\n",
       "Attributes:\n",
       "    long_name:  filtered velocity data"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 6GB\n",
       "Dimensions:     (mode: 6, zl: 96, yb: 401, xb: 1501)\n",
       "Coordinates:\n",
       "  * mode        (mode) int64 48B 0 1 2 3 4 5\n",
       "  * zl          (zl) float64 768B 2.704 8.126 13.58 ... 4.926e+03 5.029e+03\n",
       "  * xb          (xb) float64 12kB -0.0 1.0 2.0 ... 1.498e+03 1.499e+03 1.5e+03\n",
       "  * yb          (yb) float64 3kB -200.0 -199.0 -198.0 ... 198.0 199.0 200.0\n",
       "Data variables:\n",
       "    U           (mode, zl, yb, xb) float64 3GB dask.array&lt;chunksize=(1, 96, 401, 1501), meta=np.ndarray&gt;\n",
       "    W           (mode, zl, yb, xb) float64 3GB dask.array&lt;chunksize=(1, 96, 401, 1501), meta=np.ndarray&gt;\n",
       "    Wavelength  (mode, yb, xb) float64 29MB dask.array&lt;chunksize=(1, 401, 1501), meta=np.ndarray&gt;</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-8379508d-ae95-4973-9214-5c03d62a3d50' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-8379508d-ae95-4973-9214-5c03d62a3d50' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>mode</span>: 6</li><li><span class='xr-has-index'>zl</span>: 96</li><li><span class='xr-has-index'>yb</span>: 401</li><li><span class='xr-has-index'>xb</span>: 1501</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-28c0270e-fb48-4ad3-b620-f4c744f935de' class='xr-section-summary-in' type='checkbox'  checked><label for='section-28c0270e-fb48-4ad3-b620-f4c744f935de' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>mode</span></div><div class='xr-var-dims'>(mode)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5</div><input id='attrs-e677d538-4c26-47c7-a780-a1f7e56fc45a' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-e677d538-4c26-47c7-a780-a1f7e56fc45a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1b1b266e-d302-44c1-a794-0f8c16166f60' class='xr-var-data-in' type='checkbox'><label for='data-1b1b266e-d302-44c1-a794-0f8c16166f60' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0, 1, 2, 3, 4, 5])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>zl</span></div><div class='xr-var-dims'>(zl)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>2.704 8.126 ... 4.926e+03 5.029e+03</div><input id='attrs-53c23887-ba67-4ed3-99df-69ec78775b3e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-53c23887-ba67-4ed3-99df-69ec78775b3e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6e3e6b92-368a-4008-96a4-5a7bdf3dcc8e' class='xr-var-data-in' type='checkbox'><label for='data-6e3e6b92-368a-4008-96a4-5a7bdf3dcc8e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>meters</dd><dt><span>long_name :</span></dt><dd>Depth at cell center</dd><dt><span>axis :</span></dt><dd>Z</dd><dt><span>positive :</span></dt><dd>down</dd><dt><span>edges :</span></dt><dd>z_i</dd></dl></div><div class='xr-var-data'><pre>array([2.703753e+00, 8.125880e+00, 1.357918e+01, 1.906775e+01, 2.459623e+01,\n",
       "       3.016989e+01, 3.579464e+01, 4.147720e+01, 4.722515e+01, 5.304703e+01,\n",
       "       5.895252e+01, 6.495250e+01, 7.105930e+01, 7.728676e+01, 8.365052e+01,\n",
       "       9.016818e+01, 9.685954e+01, 1.037469e+02, 1.108551e+02, 1.182124e+02,\n",
       "       1.258502e+02, 1.338036e+02, 1.421120e+02, 1.508192e+02, 1.599741e+02,\n",
       "       1.696308e+02, 1.798495e+02, 1.906965e+02, 2.022449e+02, 2.145749e+02,\n",
       "       2.277741e+02, 2.419380e+02, 2.571697e+02, 2.735803e+02, 2.912884e+02,\n",
       "       3.104201e+02, 3.311080e+02, 3.534903e+02, 3.777098e+02, 4.039118e+02,\n",
       "       4.322427e+02, 4.628474e+02, 4.958665e+02, 5.314341e+02, 5.696743e+02,\n",
       "       6.106986e+02, 6.546025e+02, 7.014638e+02, 7.513394e+02, 8.042643e+02,\n",
       "       8.602503e+02, 9.192855e+02, 9.813352e+02, 1.046342e+03, 1.114229e+03,\n",
       "       1.184899e+03, 1.258243e+03, 1.334134e+03, 1.412441e+03, 1.493021e+03,\n",
       "       1.575730e+03, 1.660421e+03, 1.746950e+03, 1.835173e+03, 1.924952e+03,\n",
       "       2.016155e+03, 2.108655e+03, 2.202334e+03, 2.297081e+03, 2.392793e+03,\n",
       "       2.489374e+03, 2.586736e+03, 2.684800e+03, 2.783492e+03, 2.882746e+03,\n",
       "       2.982502e+03, 3.082706e+03, 3.183309e+03, 3.284266e+03, 3.385539e+03,\n",
       "       3.487093e+03, 3.588895e+03, 3.690919e+03, 3.793138e+03, 3.895532e+03,\n",
       "       3.998079e+03, 4.100762e+03, 4.203566e+03, 4.306477e+03, 4.409483e+03,\n",
       "       4.512572e+03, 4.615735e+03, 4.718963e+03, 4.822249e+03, 4.925586e+03,\n",
       "       5.028969e+03])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>xb</span></div><div class='xr-var-dims'>(xb)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-0.0 1.0 2.0 ... 1.499e+03 1.5e+03</div><input id='attrs-d4f2b500-1ff7-452a-a562-6c1b4d10c306' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-d4f2b500-1ff7-452a-a562-6c1b4d10c306' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-bdfb8624-1277-47c8-901a-ac40ceec24d9' class='xr-var-data-in' type='checkbox'><label for='data-bdfb8624-1277-47c8-901a-ac40ceec24d9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>km</dd><dt><span>long_name :</span></dt><dd>Distance along beam from Tasmania towards Macquarie Ridge</dd></dl></div><div class='xr-var-data'><pre>array([-0.000e+00,  1.000e+00,  2.000e+00, ...,  1.498e+03,  1.499e+03,\n",
       "        1.500e+03])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>yb</span></div><div class='xr-var-dims'>(yb)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-200.0 -199.0 ... 199.0 200.0</div><input id='attrs-34deb6c3-c0fa-4e1c-b1d1-7ae22f7daed2' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-34deb6c3-c0fa-4e1c-b1d1-7ae22f7daed2' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-70959443-9582-467a-ae3d-f07ea4ce7f97' class='xr-var-data-in' type='checkbox'><label for='data-70959443-9582-467a-ae3d-f07ea4ce7f97' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>km</dd><dt><span>long_name :</span></dt><dd>Distance perpendicular to beam referened from beam centre</dd></dl></div><div class='xr-var-data'><pre>array([-200., -199., -198., ...,  198.,  199.,  200.])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-dd000552-86b0-4bb6-b8b5-4c62893aa80f' class='xr-section-summary-in' type='checkbox'  checked><label for='section-dd000552-86b0-4bb6-b8b5-4c62893aa80f' class='xr-section-summary' >Data variables: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>U</span></div><div class='xr-var-dims'>(mode, zl, yb, xb)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(1, 96, 401, 1501), meta=np.ndarray&gt;</div><input id='attrs-22dcaced-6707-4ae8-a0a4-4beacbc72cfb' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-22dcaced-6707-4ae8-a0a4-4beacbc72cfb' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-723adf53-3321-4de0-b35c-3b402921c00d' class='xr-var-data-in' type='checkbox'><label for='data-723adf53-3321-4de0-b35c-3b402921c00d' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 2.58 GiB </td>\n",
       "                        <td> 440.85 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (6, 96, 401, 1501) </td>\n",
       "                        <td> (1, 96, 401, 1501) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 6 chunks in 25 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"381\" height=\"114\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"4\" y1=\"0\" x2=\"4\" y2=\"25\" />\n",
       "  <line x1=\"8\" y1=\"0\" x2=\"8\" y2=\"25\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"25\" />\n",
       "  <line x1=\"16\" y1=\"0\" x2=\"16\" y2=\"25\" />\n",
       "  <line x1=\"21\" y1=\"0\" x2=\"21\" y2=\"25\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >6</text>\n",
       "  <text x=\"45.412617\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,45.412617,12.706308)\">1</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"116\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"95\" y1=\"43\" x2=\"116\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"95\" y2=\"43\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"116\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 116.17561133376778,21.175611333767776 116.17561133376778,64.31787331922936 95.0,43.14226198546158\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"215\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"236\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"116\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"215\" y1=\"0\" x2=\"236\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 215.0,0.0 236.1756113337678,21.175611333767776 116.17561133376778,21.175611333767776\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"236\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"116\" y1=\"64\" x2=\"236\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"116\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"236\" y1=\"21\" x2=\"236\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"116.17561133376778,21.175611333767776 236.1756113337678,21.175611333767776 236.1756113337678,64.31787331922936 116.17561133376778,64.31787331922936\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"176.175611\" y=\"84.317873\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1501</text>\n",
       "  <text x=\"256.175611\" y=\"42.746742\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,256.175611,42.746742)\">401</text>\n",
       "  <text x=\"95.587806\" y=\"73.730068\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,95.587806,73.730068)\">96</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table></div></li><li class='xr-var-item'><div class='xr-var-name'><span>W</span></div><div class='xr-var-dims'>(mode, zl, yb, xb)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(1, 96, 401, 1501), meta=np.ndarray&gt;</div><input id='attrs-107664d3-c6b5-4755-a8a2-fd370631f474' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-107664d3-c6b5-4755-a8a2-fd370631f474' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b83242e6-a391-4a90-81f2-1c42b32d5a73' class='xr-var-data-in' type='checkbox'><label for='data-b83242e6-a391-4a90-81f2-1c42b32d5a73' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 2.58 GiB </td>\n",
       "                        <td> 440.85 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (6, 96, 401, 1501) </td>\n",
       "                        <td> (1, 96, 401, 1501) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 6 chunks in 25 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"381\" height=\"114\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"4\" y1=\"0\" x2=\"4\" y2=\"25\" />\n",
       "  <line x1=\"8\" y1=\"0\" x2=\"8\" y2=\"25\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"25\" />\n",
       "  <line x1=\"16\" y1=\"0\" x2=\"16\" y2=\"25\" />\n",
       "  <line x1=\"21\" y1=\"0\" x2=\"21\" y2=\"25\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >6</text>\n",
       "  <text x=\"45.412617\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,45.412617,12.706308)\">1</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"116\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"95\" y1=\"43\" x2=\"116\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"95\" y2=\"43\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"116\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 116.17561133376778,21.175611333767776 116.17561133376778,64.31787331922936 95.0,43.14226198546158\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"215\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"236\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"116\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"215\" y1=\"0\" x2=\"236\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 215.0,0.0 236.1756113337678,21.175611333767776 116.17561133376778,21.175611333767776\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"236\" y2=\"21\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"116\" y1=\"64\" x2=\"236\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"116\" y1=\"21\" x2=\"116\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"236\" y1=\"21\" x2=\"236\" y2=\"64\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"116.17561133376778,21.175611333767776 236.1756113337678,21.175611333767776 236.1756113337678,64.31787331922936 116.17561133376778,64.31787331922936\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"176.175611\" y=\"84.317873\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1501</text>\n",
       "  <text x=\"256.175611\" y=\"42.746742\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,256.175611,42.746742)\">401</text>\n",
       "  <text x=\"95.587806\" y=\"73.730068\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,95.587806,73.730068)\">96</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table></div></li><li class='xr-var-item'><div class='xr-var-name'><span>Wavelength</span></div><div class='xr-var-dims'>(mode, yb, xb)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(1, 401, 1501), meta=np.ndarray&gt;</div><input id='attrs-997dca3c-4332-406e-a763-cd9923fbc673' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-997dca3c-4332-406e-a763-cd9923fbc673' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d8f6f450-ab72-4a35-8abf-4d2f2bc687da' class='xr-var-data-in' type='checkbox'><label for='data-d8f6f450-ab72-4a35-8abf-4d2f2bc687da' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 27.55 MiB </td>\n",
       "                        <td> 4.59 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (6, 401, 1501) </td>\n",
       "                        <td> (1, 401, 1501) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 6 chunks in 16 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"194\" height=\"108\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"24\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"43\" x2=\"24\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"43\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"12\" y2=\"45\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"14\" y2=\"48\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"17\" y2=\"50\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"19\" y2=\"53\" />\n",
       "  <line x1=\"22\" y1=\"12\" x2=\"22\" y2=\"55\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 24.9485979497544,14.948597949754403 24.9485979497544,58.09085993521598 10.0,43.14226198546158\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"132\" y2=\"2\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"134\" y2=\"4\" />\n",
       "  <line x1=\"17\" y1=\"7\" x2=\"137\" y2=\"7\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"139\" y2=\"9\" />\n",
       "  <line x1=\"22\" y1=\"12\" x2=\"142\" y2=\"12\" />\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"144\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"24\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"130\" y1=\"0\" x2=\"144\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 130.0,0.0 144.9485979497544,14.948597949754403 24.9485979497544,14.948597949754403\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"144\" y2=\"14\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"24\" y1=\"58\" x2=\"144\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"24\" y1=\"14\" x2=\"24\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"144\" y1=\"14\" x2=\"144\" y2=\"58\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"24.9485979497544,14.948597949754403 144.9485979497544,14.948597949754403 144.9485979497544,58.09085993521598 24.9485979497544,58.09085993521598\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"84.948598\" y=\"78.090860\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1501</text>\n",
       "  <text x=\"164.948598\" y=\"36.519729\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,164.948598,36.519729)\">401</text>\n",
       "  <text x=\"7.474299\" y=\"70.616561\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,7.474299,70.616561)\">6</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table></div></li></ul></div></li><li class='xr-section-item'><input id='section-f30f0c79-b5ad-423a-ba20-a1cbc110eaf3' class='xr-section-summary-in' type='checkbox'  ><label for='section-f30f0c79-b5ad-423a-ba20-a1cbc110eaf3' class='xr-section-summary' >Indexes: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>mode</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-69095289-68c0-4eef-b3c0-837caa77ad51' class='xr-index-data-in' type='checkbox'/><label for='index-69095289-68c0-4eef-b3c0-837caa77ad51' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([0, 1, 2, 3, 4, 5], dtype=&#x27;int64&#x27;, name=&#x27;mode&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>zl</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-893eb7f2-3139-470b-b569-5a0bc4c3cbeb' class='xr-index-data-in' type='checkbox'/><label for='index-893eb7f2-3139-470b-b569-5a0bc4c3cbeb' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([ 2.703753010361301,   8.12588009790504,  13.57917597818933,\n",
       "         19.0677462815569, 24.596233863642556, 30.169888095518928,\n",
       "        35.79464280769775,   41.4772038888342, 47.225147632769605,\n",
       "        53.04703102197908, 58.952515229037026,  64.95250370685102,\n",
       "        71.05929631834977,  77.28676102065936,  83.65052465919959,\n",
       "        90.16818443278804,  96.85954154803169, 103.74685847280408,\n",
       "       110.85514100317826, 118.21244604991921, 125.85021559855585,\n",
       "       133.80363666481867, 142.11202621323997,  150.8192388847458,\n",
       "        159.9740939397846,  169.6308160172191, 179.84948209014027,\n",
       "       190.69646433279377, 202.24485548254768, 214.57485970389612,\n",
       "       227.77412800198698, 241.93801302094812, 257.16971381302045,\n",
       "        273.5802771990335, 291.28841910009595, 310.42012727064116,\n",
       "        331.1080068830699, 353.49033315796277,  377.7097814586537,\n",
       "         403.911815626739, 432.24273022650823, 462.84736177092253,\n",
       "       495.86650730936975,   531.434114639213,  569.6743347715629,\n",
       "        610.6985513861348,  654.6025206992952,  701.4637653176878,\n",
       "        751.3393646959448,  804.2642713175994,  860.2502557850537,\n",
       "        919.2855474959056,  981.3351939666219, 1046.3421157425705,\n",
       "       1114.2287902171076, 1184.8994611738667, 1258.2427449331324,\n",
       "       1334.1344904837943, 1412.4407500284888, 1493.0207265178283,\n",
       "        1575.729583438852,   1660.42102622727,  1746.949591041486,\n",
       "        1835.172602518166,   1924.95178543673, 2016.1545346247765,\n",
       "       2108.6548623278727, 2202.3340526249094, 2297.0810586950574,\n",
       "       2392.7926814860753, 2489.3735683538357,  2586.736068293191,\n",
       "        2684.799977139646,  2783.492202155834, 2882.7463711675086,\n",
       "       2982.5024072015785,  3082.706085619182, 3183.3085871598696,\n",
       "       3284.2660571827155, 3385.5391787231874, 3487.0927647655553,\n",
       "         3588.89537332429, 3690.9189474886266, 3793.1384814624917,\n",
       "       3895.5317127780127,  3998.078840228581,   4100.76226661538,\n",
       "        4203.566365092991,  4306.477267704286, 4409.4826745863365,\n",
       "        4512.571682286236,   4615.73462963141, 4718.9629596393825,\n",
       "        4822.249096016312,  4925.586332873544,  5028.968736380568],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;zl&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>xb</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-428c7a74-0193-4cc5-9c7b-77c823a5c874' class='xr-index-data-in' type='checkbox'/><label for='index-428c7a74-0193-4cc5-9c7b-77c823a5c874' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([  -0.0,    1.0,    2.0,    3.0,    4.0,    5.0,    6.0,    7.0,    8.0,\n",
       "          9.0,\n",
       "       ...\n",
       "       1491.0, 1492.0, 1493.0, 1494.0, 1495.0, 1496.0, 1497.0, 1498.0, 1499.0,\n",
       "       1500.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;xb&#x27;, length=1501))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>yb</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-c6009101-7fe8-4fa5-a03f-5a6a2c0756b0' class='xr-index-data-in' type='checkbox'/><label for='index-c6009101-7fe8-4fa5-a03f-5a6a2c0756b0' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([-200.0, -199.0, -198.0, -197.0, -196.0, -195.0, -194.0, -193.0, -192.0,\n",
       "       -191.0,\n",
       "       ...\n",
       "        191.0,  192.0,  193.0,  194.0,  195.0,  196.0,  197.0,  198.0,  199.0,\n",
       "        200.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;yb&#x27;, length=401))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-948eeb7d-5e7a-4351-8cc4-af5dad6c8df2' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-948eeb7d-5e7a-4351-8cc4-af5dad6c8df2' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 6GB\n",
       "Dimensions:     (mode: 6, zl: 96, yb: 401, xb: 1501)\n",
       "Coordinates:\n",
       "  * mode        (mode) int64 48B 0 1 2 3 4 5\n",
       "  * zl          (zl) float64 768B 2.704 8.126 13.58 ... 4.926e+03 5.029e+03\n",
       "  * xb          (xb) float64 12kB -0.0 1.0 2.0 ... 1.498e+03 1.499e+03 1.5e+03\n",
       "  * yb          (yb) float64 3kB -200.0 -199.0 -198.0 ... 198.0 199.0 200.0\n",
       "Data variables:\n",
       "    U           (mode, zl, yb, xb) float64 3GB dask.array<chunksize=(1, 96, 401, 1501), meta=np.ndarray>\n",
       "    W           (mode, zl, yb, xb) float64 3GB dask.array<chunksize=(1, 96, 401, 1501), meta=np.ndarray>\n",
       "    Wavelength  (mode, yb, xb) float64 29MB dask.array<chunksize=(1, 401, 1501), meta=np.ndarray>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmodes.interp_like(filtered.u.isel(time = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-24.04] *",
   "language": "python",
   "name": "conda-env-analysis3-24.04-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
