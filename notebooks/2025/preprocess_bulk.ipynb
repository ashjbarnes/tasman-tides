{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:42055' processes=20 threads=20, memory=416.00 GiB>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-8e57b9cf-3547-11f0-b4ad-00000558fe80</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/proxy/40717/status\" target=\"_blank\">/proxy/40717/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/proxy/40717/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">c857ef15</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"/proxy/40717/status\" target=\"_blank\">/proxy/40717/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 20\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 20\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 416.00 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-bbe5500d-ad6b-4cba-a853-a15812ddf76a</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:42055\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 20\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"/proxy/40717/status\" target=\"_blank\">/proxy/40717/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 20\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 416.00 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:46243\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/44905/status\" target=\"_blank\">/proxy/44905/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:44139\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-x0o8r_n6\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:37303\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/37589/status\" target=\"_blank\">/proxy/37589/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:35987\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-tqxg80qp\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 2</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:38591\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/45377/status\" target=\"_blank\">/proxy/45377/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:43785\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-u3wwj5te\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 3</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:43177\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/33955/status\" target=\"_blank\">/proxy/33955/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:38947\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-jvxlel9_\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 4</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:37683\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/33627/status\" target=\"_blank\">/proxy/33627/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:43761\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-5vcstwkn\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 5</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:43827\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/42725/status\" target=\"_blank\">/proxy/42725/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:42375\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-c_gpbl_7\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 6</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:41501\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/39447/status\" target=\"_blank\">/proxy/39447/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:45923\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-_soqlit1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 7</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:36263\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/43355/status\" target=\"_blank\">/proxy/43355/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:36901\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-1d24i2tw\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 8</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:34073\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/32957/status\" target=\"_blank\">/proxy/32957/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:35669\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-syu2f1xa\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 9</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:43991\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/45553/status\" target=\"_blank\">/proxy/45553/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:38019\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-wgwse_32\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 10</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:34359\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/46735/status\" target=\"_blank\">/proxy/46735/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:45899\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-4y8x7hhu\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 11</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:43817\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/41445/status\" target=\"_blank\">/proxy/41445/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:38421\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-vhwm39ez\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 12</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:36611\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/34541/status\" target=\"_blank\">/proxy/34541/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:46007\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-nndlktcy\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 13</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:44073\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/39381/status\" target=\"_blank\">/proxy/39381/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:41659\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-nswkodpb\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 14</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:43341\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/45263/status\" target=\"_blank\">/proxy/45263/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:35579\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-v90re1cc\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 15</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:35993\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/36799/status\" target=\"_blank\">/proxy/36799/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:41319\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-ia88ewpb\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 16</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:45145\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/45683/status\" target=\"_blank\">/proxy/45683/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:33285\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-b97_mnq6\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 17</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:42137\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/41861/status\" target=\"_blank\">/proxy/41861/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:39943\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-weymvb5r\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 18</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:39499\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/40649/status\" target=\"_blank\">/proxy/40649/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:44243\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-nuyz4kte\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 19</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:39737\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"/proxy/42905/status\" target=\"_blank\">/proxy/42905/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 20.80 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:43979\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /jobfs/141327948.gadi-pbs/dask-scratch-space/worker-yzpx73ol\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:42055' processes=20 threads=20, memory=416.00 GiB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/149/ab8992/tasman-tides/\")\n",
    "import xarray as xr\n",
    "import ttidelib as tt\n",
    "import scipy\n",
    "import cmocean\n",
    "import os\n",
    "from pathlib import Path\n",
    "cmap = cmocean.cm.dense_r\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "earth_cmap = matplotlib.colormaps[\"gist_earth\"]\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "# import filtering\n",
    "import numpy as np\n",
    "import dask\n",
    "dask.config.set({'logging.distributed': 'error'})\n",
    "from dask.distributed import Client,default_client\n",
    "import xrft\n",
    "\n",
    "\n",
    "client = tt.startdask(nthreads=1,n_workers = 20)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpstorage = os.getenv('PBS_JOBFS')\n",
    "tmpstorage = \"/scratch/nm03/ab8992/test\"\n",
    "outputdir = \"/scratch/nm03/ab8992/test/outputs\"\n",
    "def DirectionalFilter(data,dim = \"xb\"):\n",
    "    \"\"\"\n",
    "    Fourier filter into forward and backward propagating signals\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if \"lat\" in data.coords:\n",
    "        data = data.drop([\"lat\",\"lon\"])\n",
    "    import xrft\n",
    "    FT = xrft.fft(\n",
    "        data,dim = [\"time\",dim]\n",
    "    )\n",
    "\n",
    "    forward = np.real(xrft.ifft(\n",
    "        FT.where((FT[f\"freq_{dim}\"] > 0) & (FT.freq_time > 0), 0) +\n",
    "          FT.where((FT[f\"freq_{dim}\"] < 0) & (FT.freq_time < 0), 0),\n",
    "        dim = [\"freq_time\",f\"freq_{dim}\"]\n",
    "    ))\n",
    "\n",
    "    backward = np.real(xrft.ifft(\n",
    "        FT.where((FT[f\"freq_{dim}\"] < 0) & (FT.freq_time > 0), 0) + \n",
    "        FT.where((FT[f\"freq_{dim}\"] > 0) & (FT.freq_time < 0), 0),\n",
    "        dim = [\"freq_time\",f\"freq_{dim}\"]\n",
    "    ))\n",
    "\n",
    "    return xr.merge([forward.rename(f\"{data.name}_forward\"),backward.rename(f\"{data.name}_backward\")]).assign_coords({\"time\":data.time,dim:data[dim]})\n",
    "def laplacian(data):\n",
    "    out = data.u.bfill(\"xb\",limit = 4).ffill(\"xb\",limit = 4).differentiate(\"xb\").differentiate(\"xb\")\n",
    "    out +=data.v.bfill(\"yb\",limit = 4).ffill(\"yb\",limit = 4).differentiate(\"yb\").differentiate(\"yb\")\n",
    "    return out\n",
    "\n",
    "def _scipy_integrate(data, zl):\n",
    "    \"\"\"\n",
    "    Helper function to perform cumulative trapezoidal integration along the 'zl' axis.\n",
    "    \"\"\"\n",
    "    return scipy.integrate.cumulative_trapezoid(data, x=zl, initial=0)\n",
    "\n",
    "def scipy_integrate(data):\n",
    "    \"\"\"\n",
    "    Wrapper function to apply cumulative trapezoidal integration along the 'zl' axis.\n",
    "    \"\"\"\n",
    "    zl = data.zl.values  # Extract 'zl' coordinate values\n",
    "    return np.apply_along_axis(_scipy_integrate, data.get_axis_num('zl'), data, zl=zl)\n",
    "\n",
    "def save_temporary(expt,t0,outputdir):\n",
    "    \"\"\"\n",
    "    Open all the datasets that we need for this experiment. Save some of the simple stuff, and save the modal decompossitions to temporary storage chunked by mode for further processing.\n",
    "    \"\"\"\n",
    "    with (\n",
    "            xr.open_mfdataset(f\"/g/data/nm03/ab8992/postprocessed/{expt}/bandpassed/t0-{t0}/Filtered*.nc\",decode_times = False) as filtered,\n",
    "            tt.collect_data(expt,rawdata=[\"u\",\"v\",\"ahh\"],timerange = (filtered.time.values[0],filtered.time.values[-1])).isel(zl = slice(0,96)) as raw\n",
    "\n",
    "    ):\n",
    "        if \"40\" in expt: #! vmodes have failed for some times. For now, fix vmodes to match for each run\n",
    "            vmodes = xr.open_dataset(f\"/g/data/nm03/ab8992/postprocessed/{expt}/vertical_eigenfunctions/vmode-t0-{4216}.nc\",decode_times = False,chunks = {\"mode\":1}).isel(zl = slice(0,96))\n",
    "        else:\n",
    "            vmodes = xr.open_dataset(f\"/g/data/nm03/ab8992/postprocessed/{expt}/vertical_eigenfunctions/vmode-t0-{22000}.nc\",decode_times = False,chunks = {\"mode\":1}).isel(zl = slice(0,96))\n",
    "\n",
    "            \n",
    "        # os.remove(f\"{tmpstorage}/*.nc\")\n",
    "        vmodes = vmodes.assign_coords({\"zl\":filtered.zl})\n",
    "        ymin = vmodes.yb[0].values - 0.0001 ## This ensures random numerical changes of 1e-16 to axis values don't cause issues... \n",
    "        ymax = vmodes.yb[-1].values + 0.0001\n",
    "        raw = raw.sel(yb = slice(ymin,ymax)).assign_coords(vmodes.isel(mode = 0).coords).drop_vars([\"lat\",\"lon\"])\n",
    "        filtered = filtered.sel(yb = slice(ymin,ymax)).assign_coords(vmodes.isel(mode = 0).coords)\n",
    "\n",
    "\n",
    "        ## Here all variables are dask arrays. One by one, compute what we want and save to PBS temporart storage \n",
    "\n",
    "        for i in range(len(vmodes.mode.values)):\n",
    "            # print(\"Saving mode \",i)\n",
    "            (vmodes.U * filtered.u).fillna(0).integrate(\"zl\").rename(\"u\").isel(mode = [i]).to_netcdf(f\"{tmpstorage}/u_{i}.nc\")\n",
    "            (vmodes.U * filtered.v).fillna(0).integrate(\"zl\").rename(\"v\").isel(mode = [i]).to_netcdf(f\"{tmpstorage}/v_{i}.nc\")\n",
    "            (vmodes.W * filtered.rho).fillna(0).integrate(\"zl\").rename(\"rho\").isel(mode = [i]).to_netcdf(f\"{tmpstorage}/rho_{i}.nc\")\n",
    "\n",
    "        #! OLD VERSION: I was missing the depth in the cumsum. this should be a proper integral, \n",
    "        #! so needed to multiply by cell thicknesses\n",
    "        #! (\n",
    "        #!     vmodes.U * vmodes.W.cumsum(\"zl\") * 9.8\n",
    "        #!     ).fillna(0).integrate(\"zl\").rename(\"p_coeff\").to_netcdf(f\"{tmpstorage}/p_coeff.nc\")        \n",
    "\n",
    "        #! Updated: Now use the scipy integrate function to do cumsum correctly, retaining cell thicknesses\n",
    "        (\n",
    "            vmodes.U * scipy_integrate(vmodes.W) * 9.8\n",
    "            ).fillna(0).integrate(\"zl\").rename(\"p_coeff\").to_netcdf(f\"{tmpstorage}/p_coeff.nc\") \n",
    "\n",
    "\n",
    "        ## While we have files open, might as well save some other simple stuff, like vorticity in top 20 layers, total raw KE, both bc and bt.\n",
    "        # print(\"Save vorticity...\")\n",
    "\n",
    "        ((raw.v.differentiate(\"xb\") - raw.u.differentiate(\"yb\")).fillna(0).mean(\"time\").sel(zl = slice(0,200)).integrate(\"zl\") / 200).rename(\"vorticity\").to_netcdf(f\"{outputdir}/raw_vorticity.nc\")\n",
    "\n",
    "        print(\"Save raw KE...\")\n",
    "        (raw.u**2 + raw.v**2).fillna(0).integrate(\"zl\").mean(\"time\").rename(\"raw_KE_total\").to_netcdf(f\"{outputdir}/raw_ke_total.nc\")\n",
    "        # print(\"Save raw KE bt...\")\n",
    "        (\n",
    "            (raw.u.fillna(0).integrate(\"zl\")**2 + raw.v.fillna(0).integrate(\"zl\")**2) / raw.bathy\n",
    "        ).mean(\"time\").rename(\"raw_KE_bt\").to_netcdf(f\"{outputdir}/raw_ke_bt.nc\")\n",
    "        # print(\"Save raw dissipation...\")\n",
    "        (\n",
    "            (raw.ahh * laplacian(raw)**2).mean(\"time\") / (1e6)\n",
    "        ).fillna(0).integrate(\"zl\").rename(\"raw_dissipation\").to_netcdf(f\"{outputdir}/raw_dissipation.nc\")\n",
    "\n",
    "        # print(\"Save filtered KE...\")\n",
    "        (filtered.u**2 + filtered.v**2).fillna(0).integrate(\"zl\").mean(\"time\").rename(\"filtered_KE_total\").to_netcdf(f\"{outputdir}/filtered_ke_total.nc\")\n",
    "        # print(\"Save filtered KE bt...\")\n",
    "        (\n",
    "            (filtered.u.fillna(0).integrate(\"zl\")**2 + filtered.v.fillna(0).integrate(\"zl\")**2) / raw.bathy\n",
    "        ).mean(\"time\").rename(\"filtered_KE_bt\").to_netcdf(f\"{outputdir}/filtered_ke_bt.nc\")\n",
    "        print(\"Save filtered dissipation...\")\n",
    "        (\n",
    "            (raw.ahh * laplacian(filtered)**2).mean(\"time\") / (1e6)\n",
    "        ).fillna(0).integrate(\"zl\").rename(\"filtered_dissipation\").to_netcdf(f\"{outputdir}/filtered_dissipation.nc\")        \n",
    "\n",
    "    return\n",
    "\n",
    "def EF_from_u_rho(u,rho,p_coeff): #! TODO MAKE SURE THIS WORKS FOR NON FILTERED EF!\n",
    "    if isinstance(rho, xr.DataArray):\n",
    "        EF = rho * u * p_coeff\n",
    "        return EF.rename(\"EF\")\n",
    "\n",
    "    EF_forward = rho[f\"rho_forward\"] * u[f\"u_forward\"] * p_coeff\n",
    "    EF_backward = rho[f\"rho_backward\"] * u[f\"u_backward\"] * p_coeff\n",
    "    EF_cross = rho[f\"rho_backward\"] * u[f\"u_forward\"] * p_coeff\n",
    "    EF_cross += rho[f\"rho_forward\"] * u[f\"u_backward\"] * p_coeff\n",
    "    return xr.merge([EF_forward.rename(\"EF_alongbeam_forward\"),EF_backward.rename(\"EF_alongbeam_backward\"),EF_cross.rename(\"EF_alongbeam_xterm\")]).mean(\"time\")\n",
    "\n",
    "def EF_from_v_rho(v,rho,p_coeff): #! TODO MAKE SURE THIS WORKS FOR NON FILTERED EF!\n",
    "    if isinstance(rho, xr.DataArray):\n",
    "        EF = rho * v * p_coeff\n",
    "        return EF.rename(\"EF\")\n",
    "\n",
    "    EF_forward = rho[f\"rho_forward\"] * v[f\"v_forward\"] * p_coeff\n",
    "    EF_backward = rho[f\"rho_backward\"] * v[f\"v_backward\"] * p_coeff\n",
    "    EF_cross = rho[f\"rho_backward\"] * v[f\"v_forward\"] * p_coeff\n",
    "    EF_cross += rho[f\"rho_forward\"] * v[f\"v_backward\"] * p_coeff\n",
    "    return xr.merge([EF_forward.rename(\"EF_acrossbeam_forward\"),EF_backward.rename(\"EF_acrossbeam_backward\"),EF_cross.rename(\"EF_acrossbeam_xterm\")]).mean(\"time\")\n",
    "\n",
    "def save_modal(outputdir):\n",
    "    U = xr.open_mfdataset(f\"{tmpstorage}/u_*.nc\",combine = \"by_coords\",decode_times = False).u\n",
    "    V = xr.open_mfdataset(f\"{tmpstorage}/v_*.nc\",combine = \"by_coords\",decode_times = False).v\n",
    "    rho = xr.open_mfdataset(f\"{tmpstorage}/rho_*.nc\",combine = \"by_coords\",decode_times = False).rho\n",
    "    p_coeff = xr.open_mfdataset(f\"{tmpstorage}/p_coeff.nc\",combine = \"by_coords\",decode_times = False).p_coeff\n",
    "\n",
    "    ## EF unfiltered\n",
    "    EF_from_u_rho(U,rho,p_coeff).to_netcdf(f\"{outputdir}/EF_alongbeam.nc\")\n",
    "    EF_from_v_rho(V,rho,p_coeff).to_netcdf(f\"{outputdir}/EF_acrossbeam.nc\")\n",
    "\n",
    "    ## EF filtered\n",
    "    print(\"saving EF\")\n",
    "    with EF_from_u_rho(DirectionalFilter(U),DirectionalFilter(rho),p_coeff) as EF:\n",
    "        EF[\"EF_alongbeam_forward\"].to_netcdf(f\"{outputdir}/EF_alongbeam_forward.nc\",mode = \"w\")\n",
    "        EF[\"EF_alongbeam_backward\"].to_netcdf(f\"{outputdir}/EF_alongbeam_backward.nc\",mode = \"w\")\n",
    "        EF[\"EF_alongbeam_xterm\"].to_netcdf(f\"{outputdir}/EF_alongbeam_xterm.nc\",mode = \"w\")\n",
    "        \n",
    "    with EF_from_v_rho(DirectionalFilter(V),DirectionalFilter(rho),p_coeff) as EF:\n",
    "        EF[\"EF_acrossbeam_forward\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_forward.nc\",mode = \"w\")\n",
    "        EF[\"EF_acrossbeam_backward\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_backward.nc\",mode = \"w\")\n",
    "        EF[\"EF_acrossbeam_xterm\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_xterm.nc\",mode = \"w\")\n",
    "    return \n",
    "\n",
    "## Overwrite so we can save filtered EF\n",
    "def save_modal(outputdir):\n",
    "    U = xr.open_mfdataset(f\"{tmpstorage}/u_*.nc\",combine = \"by_coords\",decode_times = False).u\n",
    "    V = xr.open_mfdataset(f\"{tmpstorage}/v_*.nc\",combine = \"by_coords\",decode_times = False).v\n",
    "    rho = xr.open_mfdataset(f\"{tmpstorage}/rho_*.nc\",combine = \"by_coords\",decode_times = False).rho\n",
    "    p_coeff = xr.open_mfdataset(f\"{tmpstorage}/p_coeff.nc\",combine = \"by_coords\",decode_times = False).p_coeff\n",
    "\n",
    "    ## EF unfiltered\n",
    "    EF_from_u_rho(U,rho,p_coeff).to_netcdf(f\"{outputdir}/EF_alongbeam.nc\")\n",
    "    EF_from_v_rho(V,rho,p_coeff).to_netcdf(f\"{outputdir}/EF_acrossbeam.nc\")\n",
    "\n",
    "    ## EF filtered\n",
    "    with DirectionalFilter(U, dim=\"xb\") as Uf, DirectionalFilter(rho, dim=\"xb\") as rhof, DirectionalFilter(V, dim=\"xb\") as Vf:\n",
    "        EF_along = EF_from_u_rho(Uf, rhof, p_coeff)\n",
    "        EF_across = EF_from_v_rho(Vf, rhof, p_coeff)\n",
    "        KE_forward = (Uf[\"u_forward\"]**2 + Vf[\"v_forward\"]**2).mean(\"time\").rename(\"KE_forward\")\n",
    "        KE_backward = (Uf[\"u_backward\"]**2 + Vf[\"v_backward\"]**2).mean(\"time\").rename(\"KE_backward\")\n",
    "        KE_cross = (Uf[\"u_backward\"] * Uf[\"u_forward\"] + Vf[\"v_backward\"] * Vf[\"v_forward\"]).mean(\"time\").rename(\"KE_cross\")\n",
    "\n",
    "        EF_along[\"EF_alongbeam_forward\"].to_netcdf(f\"{outputdir}/EF_alongbeam_forward.nc\", mode=\"w\")\n",
    "        EF_along[\"EF_alongbeam_backward\"].to_netcdf(f\"{outputdir}/EF_alongbeam_backward.nc\", mode=\"w\")\n",
    "        EF_along[\"EF_alongbeam_xterm\"].to_netcdf(f\"{outputdir}/EF_alongbeam_xterm.nc\", mode=\"w\")\n",
    "\n",
    "        EF_across[\"EF_acrossbeam_forward\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_forward.nc\", mode=\"w\")\n",
    "        EF_across[\"EF_acrossbeam_backward\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_backward.nc\", mode=\"w\")\n",
    "        EF_across[\"EF_acrossbeam_xterm\"].to_netcdf(f\"{outputdir}/EF_acrossbeam_xterm.nc\", mode=\"w\")\n",
    "        KE_forward.to_netcdf(f\"{outputdir}/KE_forward.nc\", mode=\"w\")\n",
    "        KE_backward.to_netcdf(f\"{outputdir}/KE_backward.nc\", mode=\"w\")\n",
    "        KE_cross.to_netcdf(f\"{outputdir}/KE_cross.nc\", mode=\"w\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full-10\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Saving mode  0\n",
      "Saving mode  1\n",
      "Saving mode  2\n",
      "Saving mode  3\n",
      "Saving mode  4\n",
      "Saving mode  5\n",
      "Saving mode  6\n",
      "Saving mode  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 42.84 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save vorticity...\n",
      "Save raw KE...\n",
      "Save raw KE bt...\n",
      "Save raw dissipation...\n",
      "Save filtered KE...\n",
      "Save filtered KE bt...\n",
      "Save filtered dissipation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  full-10\n",
      "beamless-10\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Saving mode  0\n",
      "Saving mode  1\n",
      "Saving mode  2\n",
      "Saving mode  3\n",
      "Saving mode  4\n",
      "Saving mode  5\n",
      "Saving mode  6\n",
      "Saving mode  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 42.84 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save vorticity...\n",
      "Save raw KE...\n",
      "Save raw KE bt...\n",
      "Save raw dissipation...\n",
      "Save filtered KE...\n",
      "Save filtered KE bt...\n",
      "Save filtered dissipation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  beamless-10\n",
      "smooth-10\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Saving mode  0\n",
      "Saving mode  1\n",
      "Saving mode  2\n",
      "Saving mode  3\n",
      "Saving mode  4\n",
      "Saving mode  5\n",
      "Saving mode  6\n",
      "Saving mode  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 42.84 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save vorticity...\n",
      "Save raw KE...\n",
      "Save raw KE bt...\n",
      "Save raw dissipation...\n",
      "Save filtered KE...\n",
      "Save filtered KE bt...\n",
      "Save filtered dissipation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  smooth-10\n",
      "beamless-20\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Saving mode  0\n",
      "Saving mode  1\n",
      "Saving mode  2\n",
      "Saving mode  3\n",
      "Saving mode  4\n",
      "Saving mode  5\n",
      "Saving mode  6\n",
      "Saving mode  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 134.40 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save vorticity...\n",
      "Save raw KE...\n",
      "Save raw KE bt...\n",
      "Save raw dissipation...\n",
      "Save filtered KE...\n",
      "Save filtered KE bt...\n",
      "Save filtered dissipation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  beamless-20\n",
      "smooth-20\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Saving mode  0\n",
      "Saving mode  1\n",
      "Saving mode  2\n",
      "Saving mode  3\n",
      "Saving mode  4\n",
      "Saving mode  5\n",
      "Saving mode  6\n",
      "Saving mode  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 134.40 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save vorticity...\n",
      "Save raw KE...\n",
      "Save raw KE bt...\n",
      "Save raw dissipation...\n",
      "Save filtered KE...\n",
      "Save filtered KE bt...\n",
      "Save filtered dissipation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  smooth-20\n",
      "full-20\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Saving mode  0\n",
      "Saving mode  1\n",
      "Saving mode  2\n",
      "Saving mode  3\n",
      "Saving mode  4\n",
      "Saving mode  5\n",
      "Saving mode  6\n",
      "Saving mode  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 134.40 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save vorticity...\n",
      "Save raw KE...\n",
      "Save raw KE bt...\n",
      "Save raw dissipation...\n",
      "Save filtered KE...\n",
      "Save filtered KE bt...\n",
      "Save filtered dissipation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  full-20\n",
      "full-40\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Saving mode  0\n",
      "Saving mode  1\n",
      "Saving mode  2\n",
      "Saving mode  3\n",
      "Saving mode  4\n",
      "Saving mode  5\n",
      "Saving mode  6\n",
      "Saving mode  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 532.46 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save vorticity...\n",
      "Save raw KE...\n",
      "Save raw KE bt...\n",
      "Save raw dissipation...\n",
      "Save filtered KE...\n",
      "Save filtered KE bt...\n",
      "Save filtered dissipation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  full-40\n",
      "beamless-40\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Saving mode  0\n",
      "Saving mode  1\n",
      "Saving mode  2\n",
      "Saving mode  3\n",
      "Saving mode  4\n",
      "Saving mode  5\n",
      "Saving mode  6\n",
      "Saving mode  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 532.46 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save vorticity...\n",
      "Save raw KE...\n",
      "Save raw KE bt...\n",
      "Save raw dissipation...\n",
      "Save filtered KE...\n",
      "Save filtered KE bt...\n",
      "Save filtered dissipation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  beamless-40\n",
      "smooth-40\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Saving mode  0\n",
      "Saving mode  1\n",
      "Saving mode  2\n",
      "Saving mode  3\n",
      "Saving mode  4\n",
      "Saving mode  5\n",
      "Saving mode  6\n",
      "Saving mode  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 532.46 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save vorticity...\n",
      "Save raw KE...\n",
      "Save raw KE bt...\n",
      "Save raw dissipation...\n",
      "Save filtered KE...\n",
      "Save filtered KE bt...\n",
      "Save filtered dissipation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/xrft/xrft.py:573: FutureWarning: Default ifft's behaviour (lag=None) changed! Default value of lag was zero (centered output coordinates) and is now set to transformed coordinate's attribute: 'direct_lag'.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  smooth-40\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "expts = [\"full-10\",\"beamless-10\",\"smooth-10\",\"beamless-20\",\"smooth-20\",\"full-20\",\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "# expts = [\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "\n",
    "t0_20th = 22000\n",
    "t0_40th = 4216\n",
    "t0_80th = 11905\n",
    "expts = [\"full-80\"]\n",
    "for expt in expts:\n",
    "    outputdir = \"/scratch/nm03/ab8992/april25/outputs/\"+expt\n",
    "    tmpstorage = \"/scratch/nm03/ab8992/april25/tmpstorage/\"+expt\n",
    "    if os.path.exists(outputdir):\n",
    "        shutil.rmtree(outputdir)\n",
    "        shutil.rmtree(tmpstorage)\n",
    "\n",
    "    if not os.path.exists(outputdir):\n",
    "        os.makedirs(outputdir)\n",
    "    if not os.path.exists(tmpstorage):\n",
    "        os.makedirs(tmpstorage)\n",
    "    print(expt)\n",
    "    t0 = t0_20th\n",
    "    if \"40\" in expt:\n",
    "        t0 = t0_40th\n",
    "    save_temporary(expt,t0,outputdir)\n",
    "    save_modal(outputdir)\n",
    "    print(\"Done with \",expt)\n",
    "    # clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over many times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full-10\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504 already complete.\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744 already complete.\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "Done with  full-10\n",
      "Starting beamless-10\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "Done with  beamless-10\n",
      "Starting smooth-10\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "t0-10001 already complete.\n",
      "Done with  smooth-10\n",
      "Starting beamless-20\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "Done with  beamless-20\n",
      "Starting smooth-20\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-10000\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "t0-10001\n",
      "Error processing smooth-20 at time t0-10001: no files to open\n",
      "Done with  smooth-20\n",
      "Starting full-20\n",
      "t0-25704 already complete.\n",
      "t0-23544 already complete.\n",
      "t0-27864 already complete.\n",
      "t0-27504\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28224\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22200 already complete.\n",
      "t0-24984 already complete.\n",
      "t0-21744\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-28584 already complete.\n",
      "t0-23184\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-27144 already complete.\n",
      "t0-22104 already complete.\n",
      "t0-24624\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22464\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-24264 already complete.\n",
      "t0-26064\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-25344\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-23904\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22824 already complete.\n",
      "t0-26424 already complete.\n",
      "t0-26784\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Save raw KE...\n",
      "Save filtered dissipation...\n",
      "t0-22000 already complete.\n",
      "Done with  full-20\n",
      "Starting full-40\n",
      "t0-3960\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:50:28,321 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.60 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:35,739 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46373\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:45496 remote=tcp://127.0.0.1:46373>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:36,905 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35791\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:46658 remote=tcp://127.0.0.1:35791>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:38,110 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39187\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:33608 remote=tcp://127.0.0.1:39187>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:38,157 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42367\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50656 remote=tcp://127.0.0.1:42367>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:44,747 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41393\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:39278 remote=tcp://127.0.0.1:41393>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:44,823 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43701\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:58628 remote=tcp://127.0.0.1:43701>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:45,482 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.20 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:45,709 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.20 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:45,783 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39201\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:52286 remote=tcp://127.0.0.1:39201>: Stream is closed\n",
      "2025-05-19 12:50:45,950 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38909\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47338 remote=tcp://127.0.0.1:38909>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:46,008 - distributed.diskutils - ERROR - Failed to remove '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-u7bgm8or/storage' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: 'storage'\n",
      "2025-05-19 12:50:46,008 - distributed.diskutils - ERROR - Failed to remove '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-u7bgm8or' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-u7bgm8or'\n",
      "2025-05-19 12:50:46,009 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37819\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53554 remote=tcp://127.0.0.1:37819>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:46,009 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37819\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:33410 remote=tcp://127.0.0.1:37819>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:46,008 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37819\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47636 remote=tcp://127.0.0.1:37819>: Stream is closed\n",
      "2025-05-19 12:50:46,975 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45947\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:50932 remote=tcp://127.0.0.1:45947>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:50,947 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41269\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:58446 remote=tcp://127.0.0.1:41269>: Stream is closed\n",
      "2025-05-19 12:50:51,150 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40651\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:39336 remote=tcp://127.0.0.1:40651>: Stream is closed\n",
      "2025-05-19 12:50:51,149 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40651\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53490 remote=tcp://127.0.0.1:40651>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:50:54,223 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.34 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:54,573 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 4.57 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:55,498 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.25 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:59,047 - distributed.worker.memory - WARNING - Worker is at 46% memory usage. Resuming worker. Process memory: 2.42 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:59,291 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 4.43 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:50:59,448 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33225\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:38700 remote=tcp://127.0.0.1:33225>: Stream is closed\n",
      "2025-05-19 12:50:59,449 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33225\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:48376 remote=tcp://127.0.0.1:33225>: Stream is closed\n",
      "2025-05-19 12:50:59,582 - distributed.worker.memory - WARNING - Worker is at 46% memory usage. Resuming worker. Process memory: 2.41 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:01,647 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46323\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:38696 remote=tcp://127.0.0.1:46323>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:51:01,980 - distributed.worker.memory - WARNING - Worker is at 90% memory usage. Pausing worker.  Process memory: 4.69 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:02,070 - distributed.worker.memory - WARNING - Worker is at 91% memory usage. Pausing worker.  Process memory: 4.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:02,472 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 3.26 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:02,876 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 3.08 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:03,249 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.26 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-3960: Attempted to run task ('sum-sum-aggregate-getitem-getitem-862b32cb709147f01cc9d606b8f97b49', 0, 0, 0, 5) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:46323. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-10080 already complete.\n",
      "t0-9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:51:03,814 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.95 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:03,886 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.16 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:03,897 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.78 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:04,159 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.30 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:51:05,028 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.79 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:54:16,913 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38563\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:33664 remote=tcp://127.0.0.1:38563>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:54:17,909 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.20 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,071 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.19 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,336 - distributed.worker.memory - WARNING - Worker is at 60% memory usage. Resuming worker. Process memory: 3.12 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,337 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.12 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,922 - distributed.worker.memory - WARNING - Worker is at 59% memory usage. Resuming worker. Process memory: 3.11 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:18,942 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:21,041 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.35 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:21,278 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:54:22,495 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36913\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:36614 remote=tcp://127.0.0.1:36913>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-9720: Attempted to run task ('sum-sum-aggregate-getitem-getitem-c21a0a667c39bd0e2a34be7c0b73ab64', 0, 0, 0, 12) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:44399. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-5040 already complete.\n",
      "t0-6120\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:56:06,489 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.27 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:06,570 - distributed.diskutils - ERROR - Failed to remove '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-d1_305cg/storage' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: 'storage'\n",
      "2025-05-19 12:56:06,570 - distributed.diskutils - ERROR - Failed to remove '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-d1_305cg' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/jobfs/141248174.gadi-pbs/dask-scratch-space/worker-d1_305cg'\n",
      "2025-05-19 12:56:11,231 - distributed.worker.memory - WARNING - Worker is at 90% memory usage. Pausing worker.  Process memory: 4.69 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:12,018 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 3.18 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:12,547 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.18 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:14,946 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42111\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:49182 remote=tcp://127.0.0.1:42111>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:56:14,947 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42111\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:49196 remote=tcp://127.0.0.1:42111>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:56:15,128 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35213\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils.py\", line 1957, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x152ae5999390>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1535, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-05-19 12:56:19,998 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45639\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:35732 remote=tcp://127.0.0.1:45639>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:56:20,357 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.68 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-6120: Attempted to run task ('sum-sum-aggregate-getitem-getitem-4af4bcd28efe098d64066b8dcc7375bc', 0, 0, 0, 3) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45675. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4216 already complete.\n",
      "t0-10800 already complete.\n",
      "t0-9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:56:20,760 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:20,858 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.66 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:21,145 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:21,179 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:21,256 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:56:21,261 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:59:31,540 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43305\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:49308 remote=tcp://127.0.0.1:43305>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 12:59:35,747 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.19 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:59:35,986 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35115\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37182 remote=tcp://127.0.0.1:35115>: Stream is closed\n",
      "2025-05-19 12:59:39,006 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:59:39,279 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.66 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-9000: Attempted to run task ('sum-sum-aggregate-getitem-getitem-753bf4c2ff14e95838239e489b31cdf9', 0, 0, 0, 7) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:42147. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7920 already complete.\n",
      "t0-7200 already complete.\n",
      "t0-22200 already complete.\n",
      "t0-10440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:59:39,809 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.21 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 12:59:39,878 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:01:21,787 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33467\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:45126 remote=tcp://127.0.0.1:33467>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:01:27,512 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44537\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:52926 remote=tcp://127.0.0.1:44537>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:01:30,388 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.79 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-10440: Attempted to run task ('sum-sum-aggregate-getitem-getitem-835911b6f7d0d7ee2ab81e6e018cd8a1', 0, 0, 0, 5) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:41175. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6480 already complete.\n",
      "t0-5760 already complete.\n",
      "t0-4680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:01:30,610 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.65 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:30,665 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.25 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:30,741 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.77 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:30,790 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,050 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,110 - distributed.worker.memory - WARNING - Worker is at 20% memory usage. Resuming worker. Process memory: 1.07 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,193 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.34 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,242 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:31,673 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.83 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:01:32,035 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.18 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:01:32,073 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:04:32,892 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41039\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:35536 remote=tcp://127.0.0.1:41039>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:36,028 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:46095\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:37206 remote=tcp://127.0.0.1:46095>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:43,321 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42333\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:47078 remote=tcp://127.0.0.1:42333>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:43,321 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42333\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:47082 remote=tcp://127.0.0.1:42333>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:43,660 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:39287\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42314 remote=tcp://127.0.0.1:39287>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:04:48,907 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:04:48,923 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.79 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-4680: Attempted to run task ('sum-sum-aggregate-getitem-getitem-14219f83c476d5354ad4b70c43e5e9be', 0, 0, 0, 12) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:44817. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:04:49,178 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:04:49,307 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:04:49,432 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:04:49,523 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:06:34,115 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42455\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:33394 remote=tcp://127.0.0.1:42455>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:06:38,131 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:34689\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:56550 remote=tcp://127.0.0.1:34689>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-6840: Attempted to run task ('sum-sum-aggregate-getitem-getitem-170264c4273018b00c6006f29e93cd50', 0, 0, 0, 7) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:33275. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4320 already complete.\n",
      "t0-5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:06:46,112 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.97 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,188 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.84 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,226 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,558 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.32 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,609 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,732 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.34 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,752 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 4.46 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,787 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.77 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:06:46,827 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:07:49,576 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47086 remote=tcp://127.0.0.1:42179>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:07:49,576 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47060 remote=tcp://127.0.0.1:42179>: Stream is closed\n",
      "2025-05-19 13:07:49,577 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47080 remote=tcp://127.0.0.1:42179>: Stream is closed\n",
      "2025-05-19 13:07:49,577 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:47098 remote=tcp://127.0.0.1:42179>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:07:49,577 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42179\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:47076 remote=tcp://127.0.0.1:42179>: Stream is closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-5400: Attempted to run task ('sum-sum-aggregate-getitem-getitem-1a015dc65063753eb6bf2363d6d2a0a5', 0, 0, 0, 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:36485. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-9360 already complete.\n",
      "t0-8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:08:04,113 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.83 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:08:04,537 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 4.38 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:08:04,609 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:09:33,549 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:37735\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:56088 remote=tcp://127.0.0.1:37735>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:09:35,436 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:09:44,947 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-8280: Attempted to run task ('sum-sum-aggregate-getitem-getitem-bb01fd8c6d65850bdb8896ad0081cbe7', 0, 0, 0, 4) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:39927. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:09:45,377 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.21 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:09:45,446 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:10:36,002 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45707\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils.py\", line 1957, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x148128076800>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1535, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-05-19 13:10:38,985 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41879\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:54474 remote=tcp://127.0.0.1:41879>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:10:39,830 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.82 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing full-40 at time t0-7560: Attempted to run task ('sum-sum-aggregate-getitem-getitem-4f3a828dc2042670a1d7c47e35ae03d3', 0, 0, 0, 1) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45555. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-8640 already complete.\n",
      "t0-4416 already complete.\n",
      "Done with  full-40\n",
      "Starting beamless-40\n",
      "t0-3960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:10:40,295 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 4.41 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:10:40,331 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:10:40,524 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:10:41,121 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:10:41,124 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:11:45,649 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45583\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:45530 remote=tcp://127.0.0.1:45583>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:11:45,649 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45583\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:45518 remote=tcp://127.0.0.1:45583>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:11:55,676 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38937\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:55928 remote=tcp://127.0.0.1:38937>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:11:55,676 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38937\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:55926 remote=tcp://127.0.0.1:38937>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:11:55,676 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38937\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:55912 remote=tcp://127.0.0.1:38937>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:12:01,393 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.77 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-3960: Attempted to run task ('sum-sum-aggregate-getitem-getitem-629d41d228a0464c12f02845ae05a77f', 0, 0, 0, 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:46405. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-10080 already complete.\n",
      "t0-9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:12:01,636 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.87 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:01,912 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.34 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:01,956 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.84 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:01,993 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,037 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,124 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.27 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,134 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,207 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.21 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,250 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 4.38 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,253 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:12:02,337 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:15:03,704 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33879\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:38278 remote=tcp://127.0.0.1:33879>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:15:03,704 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33879\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:38274 remote=tcp://127.0.0.1:33879>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:15:08,741 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38647\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:58270 remote=tcp://127.0.0.1:38647>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-9720: Attempted to run task ('sum-sum-aggregate-getitem-getitem-6297ab17a3b92c169fa02a03b388139a', 0, 0, 0, 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:41053. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-5040 already complete.\n",
      "t0-6120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:15:08,812 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.69 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:17:03,100 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:35239\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils.py\", line 1957, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 559, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x145dbf2eebf0>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 366, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/asyncio/tasks.py\", line 605, in sleep\n",
      "    return await future\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1535, in connect\n",
      "    raise CommClosedError(reason)\n",
      "distributed.comm.core.CommClosedError: Address removed.\n",
      "2025-05-19 13:17:05,740 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-6120: Attempted to run task ('sum-sum-aggregate-getitem-getitem-9ae6ead6ef416386c84e42abc7f019c2', 0, 0, 0, 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:43223. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4216 already complete.\n",
      "t0-10800 already complete.\n",
      "t0-9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:17:06,133 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.71 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,181 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.69 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,265 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 4.43 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,338 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,375 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.67 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,945 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:17:06,975 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing beamless-40 at time t0-9000: Attempted to run task ('sum-sum-aggregate-getitem-getitem-7e34bb5b63bd9345a16c988bb30a5fdf', 0, 0, 0, 3) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:34701. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7920 already complete.\n",
      "t0-7200 already complete.\n",
      "t0-22200\n",
      "Error processing beamless-40 at time t0-22200: no files to open\n",
      "t0-10440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:18:24,196 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.91 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,308 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.67 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,383 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,508 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.32 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,590 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.26 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,593 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,708 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,768 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,924 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.26 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:24,983 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:25,156 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.21 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:18:25,167 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing beamless-40 at time t0-10440: Attempted to run task ('sum-sum-aggregate-getitem-getitem-51ccdd96af9f230bbff57a99f594b3d4', 0, 0, 0, 3) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:37685. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6480 already complete.\n",
      "t0-5760 already complete.\n",
      "t0-4680\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:22:19,192 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43391\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42478 remote=tcp://127.0.0.1:43391>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-4680: Attempted to run task ('sum-sum-aggregate-getitem-getitem-943b09550ed5020bd65b08492dfc11a7', 0, 0, 0, 5) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:37923. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:22:27,221 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,276 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.77 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,420 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.68 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,804 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 4.40 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,875 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,887 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.27 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,913 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.35 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,919 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:22:27,920 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:25:16,373 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44903\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44744 remote=tcp://127.0.0.1:44903>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:16,373 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44903\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:44724 remote=tcp://127.0.0.1:44903>: Stream is closed\n",
      "2025-05-19 13:25:16,373 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44903\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44734 remote=tcp://127.0.0.1:44903>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:16,373 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44903\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44738 remote=tcp://127.0.0.1:44903>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:19,039 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40109\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:55664 remote=tcp://127.0.0.1:40109>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:20,746 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33065\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:60442 remote=tcp://127.0.0.1:33065>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:25:20,990 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:40197\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:59788 remote=tcp://127.0.0.1:40197>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-6840: Attempted to run task ('sum-sum-aggregate-getitem-getitem-c4413a1a9e2f7b72653924bf071fc1ac', 0, 0, 0, 11) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:40197. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4320 already complete.\n",
      "t0-5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:25:21,446 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.66 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:21,479 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.81 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:21,909 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:21,920 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:21,986 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.29 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,080 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,121 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 4.32 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,146 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:25:22,558 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.68 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,819 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.24 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:25:22,958 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:28:03,904 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45609\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42122 remote=tcp://127.0.0.1:45609>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:28:03,904 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45609\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:42132 remote=tcp://127.0.0.1:45609>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:28:17,288 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.84 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-5400: Attempted to run task ('sum-sum-aggregate-getitem-getitem-3dad12905b35ca4668aafb5e727ee377', 0, 0, 0, 5) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45701. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-9360 already complete.\n",
      "t0-8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:28:17,611 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.65 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:28:17,825 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.67 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:28:17,841 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 4.43 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:28:17,887 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 3.75 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:29:54,943 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44943\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:59032 remote=tcp://127.0.0.1:44943>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:30:05,021 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41563\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:38798 remote=tcp://127.0.0.1:41563>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:30:08,096 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-8280: Attempted to run task ('sum-sum-aggregate-getitem-getitem-261695895cc976ca757ce3e7b9189a00', 0, 0, 0, 10) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:35709. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:30:08,305 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:30:08,555 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 4.31 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:30:08,601 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:30:08,688 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.19 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:30:08,694 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.74 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:32:42,644 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45485\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44244 remote=tcp://127.0.0.1:45485>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:32:42,645 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45485\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:44234 remote=tcp://127.0.0.1:45485>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:32:44,828 - distributed.worker.memory - WARNING - Worker is at 92% memory usage. Pausing worker.  Process memory: 4.82 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:32:45,797 - distributed.worker.memory - WARNING - Worker is at 50% memory usage. Resuming worker. Process memory: 2.65 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing beamless-40 at time t0-7560: Attempted to run task ('sum-sum-aggregate-getitem-getitem-e7b50aa701f4ddcf434551e57119eed1', 0, 0, 0, 9) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45339. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-8640 already complete.\n",
      "t0-4416 already complete.\n",
      "Done with  beamless-40\n",
      "Starting smooth-40\n",
      "t0-3960\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing smooth-40 at time t0-3960: Attempted to run task ('sum-sum-aggregate-getitem-getitem-1e9598d4bd36435d1622eb28908ba1b7', 0, 0, 0, 4) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:40305. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-10080 already complete.\n",
      "t0-9720\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:35:08,453 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.76 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-9720: Attempted to run task ('sum-sum-aggregate-getitem-getitem-82cb470dcab40375017b573447a6fea4', 0, 0, 0, 2) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:37017. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-5040\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:35:54,858 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:34335\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:40434 remote=tcp://127.0.0.1:34335>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-5040: Attempted to run task ('sum-sum-aggregate-getitem-getitem-a5f7f29bd3cf6bd8e72b95803ef2973f', 0, 0, 0, 8) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:37895. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6120\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:37:19,809 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:36677\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:46336 remote=tcp://127.0.0.1:36677>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:37:27,143 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38449\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 227, in read\n",
      "    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n",
      "    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2869, in get_data_from_worker\n",
      "    response = await send_recv(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://127.0.0.1:53176 remote=tcp://127.0.0.1:38449>: Stream is closed\n",
      "2025-05-19 13:37:36,120 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-6120: Attempted to run task ('sum-sum-aggregate-getitem-getitem-103c9c09271e7aaa8a287b2da5d363a8', 0, 0, 0, 6) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:46633. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4216 already complete.\n",
      "t0-10800 already complete.\n",
      "t0-9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:37:36,452 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.91 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:37:36,489 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 4.19 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:37:36,518 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.71 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:37:36,604 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.22 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:37:36,751 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 3.50 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing smooth-40 at time t0-9000: Attempted to run task ('sum-sum-aggregate-getitem-getitem-f9b873c86fb1ee51efaf192dd32ed9c8', 0, 0, 0, 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:33039. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-7920 already complete.\n",
      "t0-7200 already complete.\n",
      "t0-22200\n",
      "Error processing smooth-40 at time t0-22200: no files to open\n",
      "t0-10440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:39:31,644 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:39:32,415 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.88 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:39:32,787 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:39:32,815 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n",
      "Error processing smooth-40 at time t0-10440: Attempted to run task ('sum-sum-aggregate-getitem-getitem-458fddef5f4dd45cae0c972ef93ca987', 0, 0, 0, 2) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:45495. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6480 already complete.\n",
      "t0-5760\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:41:53,115 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44237\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:33836 remote=tcp://127.0.0.1:44237>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-5760: Attempted to run task ('sum-sum-aggregate-getitem-getitem-98c72329c6faaf41ff4717ee51c0390b', 0, 0, 0, 2) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:40325. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:42:08,491 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.71 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:42:09,025 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.23 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:42:09,087 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.73 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:44:58,285 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33397\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:57388 remote=tcp://127.0.0.1:33397>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-4680: Attempted to run task ('sum-sum-aggregate-getitem-getitem-718027bb74e37b315f481a6472623327', 0, 0, 0, 7) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:41791. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-6840\n",
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:46:00,806 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41039\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:39246 remote=tcp://127.0.0.1:41039>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:46:01,067 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38877\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:58682 remote=tcp://127.0.0.1:38877>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:46:09,329 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.66 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing smooth-40 at time t0-6840: Attempted to run task ('sum-sum-aggregate-getitem-getitem-e4f0dea7dc72d3cc34079265f0eeecdd', 0, 0, 0, 3) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:42307. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n",
      "t0-4320 already complete.\n",
      "t0-5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:46:09,895 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 4.24 GiB -- Worker memory limit: 5.20 GiB\n",
      "2025-05-19 13:46:09,926 - distributed.worker.memory - WARNING - Worker is at 71% memory usage. Resuming worker. Process memory: 3.72 GiB -- Worker memory limit: 5.20 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading u...\tdone.\n",
      "loading v...\tdone.\n",
      "loading ahh...\tdone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:47:08,000 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:33599\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:37802 remote=tcp://127.0.0.1:33599>: ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "2025-05-19 13:47:11,002 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:38583\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n",
      "    bytes_read = self.read_from_fd(buf)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n",
      "    return self.socket.recv_into(buf, len(buf))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2059, in gather_dep\n",
      "    response = await get_data_from_worker(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 2866, in get_data_from_worker\n",
      "    comm = await rpc.connect(worker)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1533, in connect\n",
      "    return connect_attempt.result()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1423, in _connect\n",
      "    comm = await connect(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/core.py\", line 377, in connect\n",
      "    handshake = await comm.read()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:51060 remote=tcp://127.0.0.1:38583>: ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 13:49:33,733 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name=\"execute(('sum-sum-aggregate-getitem-getitem-bf494ad25202c156026f26d0ce635219', 0, 0, 0, 0))\" coro=<Worker.execute() done, defined at /g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError\n",
      "2025-05-19 13:49:33,974 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:57958 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,975 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56538 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,977 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:50398 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,977 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56606 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,998 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:35756 remote=tcp://127.0.0.1:38065>: Stream is closed\n",
      "2025-05-19 13:49:33,996 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 225, in read\n",
      "    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/worker.py\", line 1250, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 459, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/utils_comm.py\", line 438, in retry\n",
      "    return await coro()\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1254, in send_recv_from_rpc\n",
      "    return await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/core.py\", line 1013, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 236, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/g/data/hh5/public/apps/miniconda3/envs/analysis3-24.04/lib/python3.10/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:50484 remote=tcp://127.0.0.1:38065>: Stream is closed\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "expts = [\"full-10\",\"beamless-10\",\"smooth-10\",\"beamless-20\",\"smooth-20\",\"full-20\",\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "# expts = [\"full-40\",\"beamless-40\",\"smooth-40\"]\n",
    "\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "t0_20th = 22000\n",
    "t0_40th = 4216\n",
    "for expt in expts:\n",
    "    print(f\"Starting {expt}\" )\n",
    "\n",
    "    times = os.listdir(f\"/g/data/nm03/ab8992/postprocessed/{expt}/bandpassed/\")\n",
    "    for t0 in times:\n",
    "        outputdir = f\"/scratch/nm03/ab8992/april-manytimes/{expt}/{t0}\"\n",
    "        # CHeck how many files are in the directory\n",
    "        if os.path.exists(outputdir) and len(os.listdir(outputdir)) == 18:\n",
    "            print(f\"{t0} already complete.\")\n",
    "            continue\n",
    "        print(t0)\n",
    "        tmpstorage = os.getenv('PBS_JOBFS') + \"/tmpstorage\"\n",
    "        # tmpstorage = \"/scratch/nm03/ab8992/april-tmpstorage/tmpstorage/\"+expt\n",
    "        if os.path.exists(tmpstorage):\n",
    "            shutil.rmtree(tmpstorage)\n",
    "        if os.path.exists(outputdir):\n",
    "            shutil.rmtree(outputdir)\n",
    "        os.makedirs(outputdir)\n",
    "        os.makedirs(tmpstorage)\n",
    "       \n",
    "        try:\n",
    "            save_temporary(expt, t0.split(\"-\")[-1], outputdir)\n",
    "            save_modal(outputdir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {expt} at time {t0}: {e}\")\n",
    "    print(\"Done with \",expt)\n",
    "    # clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-24.04] *",
   "language": "python",
   "name": "conda-env-analysis3-24.04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
